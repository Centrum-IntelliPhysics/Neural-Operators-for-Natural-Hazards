{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-TdAyUB0JL5"
   },
   "source": [
    "#### Designing a DeepONet framework for mapping the ground acceleration to the structural response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiytiWyl0JL6",
    "outputId": "a6fe1eea-c14d-4d04-b88c-66444b33bbe4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy.random as npr\n",
    "from jax import jit, grad, vmap\n",
    "from jax.example_libraries.optimizers import adam\n",
    "from jax import value_and_grad\n",
    "from functools import partial\n",
    "from jax import jacfwd, jacrev\n",
    "import jax.nn as jnn\n",
    "import math\n",
    "from jax import random\n",
    "import jax\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from flax import linen as nn\n",
    "from jax.lax import conv_general_dilated as conv_lax\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from termcolor import colored\n",
    "from scipy.io import loadmat\n",
    "import scipy.io as io\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Check where gpu is enable or not\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)\n",
    "\n",
    "cluster = False\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaGMlhJl0JL7",
    "outputId": "5db466fa-897f-4405-969f-ad31658b8fa3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "if cluster == True:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-seed', dest='seed', type=int, default=0, help='Seed number.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Print all the arguments\n",
    "    for arg in vars(args):\n",
    "        print(f'{arg}: {getattr(args, arg)}')\n",
    "\n",
    "    seed = args.seed\n",
    "\n",
    "if cluster == False:\n",
    "    seed = 0 # Seed number.\n",
    "\n",
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(), 'Results')\n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "\n",
    "if save == True and cluster == True:\n",
    "    orig_stdout = sys.stdout\n",
    "    q = open(os.path.join(resultdir, 'outputs.txt'), 'w')\n",
    "    sys.stdout = q\n",
    "    print (\"------START------\")\n",
    "\n",
    "print('seed = '+str(seed))\n",
    "np.random.seed(seed)\n",
    "key = 1234 #random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_matrix(matrix, num_subparts=118):\n",
    "    \n",
    "    subpart_size = matrix.shape[1] // num_subparts\n",
    "    divided_matrix = matrix.reshape(matrix.shape[0] * num_subparts, subpart_size)\n",
    "    \n",
    "    return divided_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVE4R7ox0JL7",
    "outputId": "74110ef4-29c0-4e48-911f-675b537590ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the grid: (5980, 1)\n",
      "Shape of inputs_train: (800, 5980)\n",
      "Shape of inputs_test: (200, 5980)\n",
      "Shape of outputs_train: (800, 5980)\n",
      "Shape of outputs_test: (200, 5980)\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = loadmat('../data/dataset_EQ.mat') # Load the .mat file\n",
    "indices = io.loadmat('../data/train_test_index.mat')\n",
    "index_train = indices['train'].T\n",
    "index_test = indices['test'].T\n",
    "\n",
    "inputs_train = data['ground_motion'][index_train[:,0]-1,21::]\n",
    "outputs_train = data['displacement'][index_train[:,0]-1,21::,5]\n",
    "inputs_test = data['ground_motion'][index_test[:,0]-1,21::]\n",
    "outputs_test = data['displacement'][index_test[:,0]-1,21::,5]\n",
    "\n",
    "#inputs_train = divide_matrix(inputs_train)\n",
    "#inputs_test = divide_matrix(inputs_test)\n",
    "#outputs_train = divide_matrix(outputs_train)\n",
    "#outputs_test = divide_matrix(outputs_test)\n",
    "\n",
    "num_train = inputs_train.shape[0]\n",
    "num_test = inputs_test.shape[0]\n",
    "\n",
    "outputs_mean = np.mean(outputs_train, axis=0)\n",
    "outputs_std = np.std(outputs_train, axis=0)\n",
    "outputs_train = (outputs_train - outputs_mean)/outputs_std\n",
    "outputs_test = (outputs_test - outputs_mean)/outputs_std\n",
    "\n",
    "inputs_mean = np.mean(inputs_train, axis=0)\n",
    "inputs_std = np.std(inputs_train, axis=0)\n",
    "inputs_train = (inputs_train - inputs_mean)/inputs_std\n",
    "inputs_test = (inputs_test - inputs_mean)/inputs_std\n",
    "\n",
    "num_t = inputs_train.shape[-1]\n",
    "grid = np.array(np.linspace(0,1,num_t)).reshape(num_t,1)\n",
    "print(\"Shape of the grid:\", grid.shape)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-UHALcEH0JL7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Glorot (Xavier) normal distribution for weight initialization\n",
    "initializer = jax.nn.initializers.glorot_normal()\n",
    "\n",
    "def init_glorot_params(layer_sizes, key = random.PRNGKey(seed)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the neural network using Glorot (Xavier) initialization.\n",
    "\n",
    "    Args:\n",
    "    layer_sizes (list): List of integers representing the size of each layer.\n",
    "    key (PRNGKey): Random number generator key for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples, each containing weights and biases for a layer.\n",
    "    \"\"\"\n",
    "    return [(initializer(key, (m, n), jnp.float32), jnp.zeros(n))\n",
    "            for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def BranchNet(params, x):\n",
    "    \"\"\"\n",
    "    Implement the branch network of the DeepONet.\n",
    "\n",
    "    Args:\n",
    "    params (list): List of weight and bias tuples for each layer.\n",
    "    x (array): Input to the branch network.\n",
    "\n",
    "    Returns:\n",
    "    array: Output of the branch network.\n",
    "    \"\"\"\n",
    "    def single_forward(params, x):\n",
    "        for w, b in params:\n",
    "            outputs = jnp.dot(x, w) + b\n",
    "            x = jnn.silu(outputs)\n",
    "        return outputs\n",
    "\n",
    "    return vmap(partial(single_forward, params))(x)\n",
    "\n",
    "def TrunkNet(params, x):\n",
    "    \"\"\"\n",
    "    Implement the trunk network of the DeepONet.\n",
    "\n",
    "    Args:\n",
    "    params (list): List of weight and bias tuples for each layer.\n",
    "    x (float): First input to the trunk network.\n",
    "    t (float): Second input to the trunk network.\n",
    "\n",
    "    Returns:\n",
    "    array: Output of the trunk network.\n",
    "    \"\"\"\n",
    "    inputs = jnp.array(x)\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(x, w) + b\n",
    "        x = jnn.silu(outputs)\n",
    "    return outputs\n",
    "\n",
    "@jit\n",
    "def DeepONet(params, branch_inputs, trunk_inputs):\n",
    "    \"\"\"\n",
    "    Implement the complete DeepONet architecture.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Tuple containing branch and trunk network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "\n",
    "    Returns:\n",
    "    array: Output of the DeepONet.\n",
    "    \"\"\"\n",
    "    params_branch, params_trunk = params\n",
    "    branch_outputs = lambda x: BranchNet(params_branch, x)\n",
    "    b_out = branch_outputs(branch_inputs)\n",
    "    trunk_output = lambda y: TrunkNet(params_trunk, y)\n",
    "    t_out = trunk_output(trunk_inputs)\n",
    "    results = jnp.einsum('ik, lk -> il',b_out, t_out)\n",
    "    return results\n",
    "\n",
    "# network parameters.\n",
    "p = 100 # Number of output neurons in both the branch and trunk net outputs.\n",
    "nx = inputs_train.shape[-1]\n",
    "input_neurons_branch = nx # m\n",
    "input_neurons_trunk = 1\n",
    "\n",
    "layer_sizes_b = [input_neurons_branch] + [100]*6 + [p]\n",
    "layer_sizes_t = [input_neurons_trunk] + [100]*6 + [p]\n",
    "\n",
    "params_branch = init_glorot_params(layer_sizes=layer_sizes_b)\n",
    "params_trunk = init_glorot_params(layer_sizes=layer_sizes_t)\n",
    "\n",
    "params= (params_branch, params_trunk)\n",
    "\n",
    "def objective(params, branch_inputs, trunk_inputs, target_values):\n",
    "    \"\"\"\n",
    "    Define the objective function (loss function) for training.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Tuple containing branch and trunk network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "    target_values (array): True output values to compare against.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean squared error loss.\n",
    "    \"\"\"\n",
    "    predictions = DeepONet(params, branch_inputs, trunk_inputs)\n",
    "    loss_mse = jnp.mean((predictions - target_values)**2)\n",
    "    return loss_mse\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "@jit\n",
    "def resnet_update(params, branch_input, trunk_inputs, target_values, opt_state):\n",
    "    \"\"\"\n",
    "    Compute the gradient for a batch and update the parameters.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Current network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "    target_values (array): True output values.\n",
    "    opt_state: Current state of the optimizer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Updated parameters, updated optimizer state, and current loss value.\n",
    "    \"\"\"\n",
    "    value, grads = value_and_grad(objective)(params, branch_input, trunk_inputs, target_values)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "opt_init, opt_update, get_params = adam(step_size=1e-3, b1=0.9, b2=0.999, eps=1e-08)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "bs = 500 #batch size\n",
    "iteration_list, loss_list, test_loss_list = [], [], []\n",
    "iteration = 0\n",
    "\n",
    "n_epochs = 1000000\n",
    "num_samples = len(inputs_train)\n",
    "\n",
    "# test input preparation\n",
    "branch_inputs_test = inputs_test\n",
    "targets = outputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NBe_opRm0JL8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model_params(params, resultdir, filename='model_params_deeponet.pkl'):\n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "\n",
    "    save_path = os.path.join(resultdir, filename)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "def load_model_params(resultdir, filename='model_params_deeponet.pkl'):\n",
    "    load_path = os.path.join(resultdir, filename)\n",
    "    with open(load_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    return params\n",
    "\n",
    "# Saving\n",
    "if save:\n",
    "    save_model_params(params, resultdir)\n",
    "\n",
    "# Loading (uncomment when needed)\n",
    "# model_params = load_model_params(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K_cWIcoG0JL8",
    "outputId": "14ed2ce1-0a8a-4013-b6ef-bde0813eba52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initial model at iteration 0\n",
      "New best model saved at iteration 0 with test MSE: 1.0000139\n",
      "Iteration:   0, Train loss: 1.0000139, Test loss: 1.0000139, Best test loss: 1.0000139, Time: 5.75\n",
      "New best model saved at iteration 1000 with test MSE: 0.6709566\n",
      "Iteration: 1000, Train loss: 0.6709566, Test loss: 0.6709566, Best test loss: 0.6709566, Time: 38.72\n",
      "New best model saved at iteration 2000 with test MSE: 0.6702158\n",
      "Iteration: 2000, Train loss: 0.6702158, Test loss: 0.6702158, Best test loss: 0.6702158, Time: 71.82\n",
      "New best model saved at iteration 3000 with test MSE: 0.6564698\n",
      "Iteration: 3000, Train loss: 0.6564698, Test loss: 0.6564698, Best test loss: 0.6564698, Time: 104.93\n",
      "New best model saved at iteration 4000 with test MSE: 0.6300822\n",
      "Iteration: 4000, Train loss: 0.6300822, Test loss: 0.6300822, Best test loss: 0.6300822, Time: 138.02\n",
      "New best model saved at iteration 5000 with test MSE: 0.5968205\n",
      "Iteration: 5000, Train loss: 0.5968205, Test loss: 0.5968205, Best test loss: 0.5968205, Time: 171.14\n",
      "New best model saved at iteration 6000 with test MSE: 0.5371862\n",
      "Iteration: 6000, Train loss: 0.5371862, Test loss: 0.5371862, Best test loss: 0.5371862, Time: 204.16\n",
      "New best model saved at iteration 7000 with test MSE: 0.4265080\n",
      "Iteration: 7000, Train loss: 0.4265080, Test loss: 0.4265080, Best test loss: 0.4265080, Time: 237.25\n",
      "New best model saved at iteration 8000 with test MSE: 0.3527492\n",
      "Iteration: 8000, Train loss: 0.3527492, Test loss: 0.3527492, Best test loss: 0.3527492, Time: 270.39\n",
      "New best model saved at iteration 9000 with test MSE: 0.2850462\n",
      "Iteration: 9000, Train loss: 0.2850462, Test loss: 0.2850462, Best test loss: 0.2850462, Time: 303.44\n",
      "New best model saved at iteration 10000 with test MSE: 0.2374889\n",
      "Iteration: 10000, Train loss: 0.2374889, Test loss: 0.2374889, Best test loss: 0.2374889, Time: 336.49\n",
      "New best model saved at iteration 11000 with test MSE: 0.1953275\n",
      "Iteration: 11000, Train loss: 0.1953275, Test loss: 0.1953275, Best test loss: 0.1953275, Time: 369.49\n",
      "New best model saved at iteration 12000 with test MSE: 0.1657800\n",
      "Iteration: 12000, Train loss: 0.1657800, Test loss: 0.1657800, Best test loss: 0.1657800, Time: 403.20\n",
      "New best model saved at iteration 13000 with test MSE: 0.1477262\n",
      "Iteration: 13000, Train loss: 0.1477262, Test loss: 0.1477262, Best test loss: 0.1477262, Time: 436.87\n",
      "New best model saved at iteration 14000 with test MSE: 0.1358174\n",
      "Iteration: 14000, Train loss: 0.1358174, Test loss: 0.1358174, Best test loss: 0.1358174, Time: 470.55\n",
      "New best model saved at iteration 15000 with test MSE: 0.1250273\n",
      "Iteration: 15000, Train loss: 0.1250273, Test loss: 0.1250273, Best test loss: 0.1250273, Time: 504.63\n",
      "New best model saved at iteration 16000 with test MSE: 0.1148925\n",
      "Iteration: 16000, Train loss: 0.1148925, Test loss: 0.1148925, Best test loss: 0.1148925, Time: 539.05\n",
      "New best model saved at iteration 17000 with test MSE: 0.1084515\n",
      "Iteration: 17000, Train loss: 0.1084515, Test loss: 0.1084515, Best test loss: 0.1084515, Time: 573.65\n",
      "New best model saved at iteration 18000 with test MSE: 0.1013213\n",
      "Iteration: 18000, Train loss: 0.1013213, Test loss: 0.1013213, Best test loss: 0.1013213, Time: 608.59\n",
      "New best model saved at iteration 19000 with test MSE: 0.0943149\n",
      "Iteration: 19000, Train loss: 0.0943149, Test loss: 0.0943149, Best test loss: 0.0943149, Time: 643.73\n",
      "New best model saved at iteration 20000 with test MSE: 0.0903085\n",
      "Iteration: 20000, Train loss: 0.0903085, Test loss: 0.0903085, Best test loss: 0.0903085, Time: 679.20\n",
      "New best model saved at iteration 21000 with test MSE: 0.0845694\n",
      "Iteration: 21000, Train loss: 0.0845694, Test loss: 0.0845694, Best test loss: 0.0845694, Time: 714.09\n",
      "New best model saved at iteration 22000 with test MSE: 0.0814551\n",
      "Iteration: 22000, Train loss: 0.0814551, Test loss: 0.0814551, Best test loss: 0.0814551, Time: 749.68\n",
      "New best model saved at iteration 23000 with test MSE: 0.0791193\n",
      "Iteration: 23000, Train loss: 0.0791193, Test loss: 0.0791193, Best test loss: 0.0791193, Time: 785.14\n",
      "New best model saved at iteration 24000 with test MSE: 0.0755422\n",
      "Iteration: 24000, Train loss: 0.0755422, Test loss: 0.0755422, Best test loss: 0.0755422, Time: 821.16\n",
      "New best model saved at iteration 25000 with test MSE: 0.0729527\n",
      "Iteration: 25000, Train loss: 0.0729527, Test loss: 0.0729527, Best test loss: 0.0729527, Time: 856.03\n",
      "New best model saved at iteration 26000 with test MSE: 0.0717384\n",
      "Iteration: 26000, Train loss: 0.0717384, Test loss: 0.0717384, Best test loss: 0.0717384, Time: 892.27\n",
      "New best model saved at iteration 27000 with test MSE: 0.0684350\n",
      "Iteration: 27000, Train loss: 0.0684350, Test loss: 0.0684350, Best test loss: 0.0684350, Time: 927.44\n",
      "New best model saved at iteration 28000 with test MSE: 0.0668413\n",
      "Iteration: 28000, Train loss: 0.0668413, Test loss: 0.0668413, Best test loss: 0.0668413, Time: 962.69\n",
      "New best model saved at iteration 29000 with test MSE: 0.0656283\n",
      "Iteration: 29000, Train loss: 0.0656283, Test loss: 0.0656283, Best test loss: 0.0656283, Time: 998.19\n",
      "New best model saved at iteration 30000 with test MSE: 0.0642539\n",
      "Iteration: 30000, Train loss: 0.0642539, Test loss: 0.0642539, Best test loss: 0.0642539, Time: 1032.62\n",
      "New best model saved at iteration 31000 with test MSE: 0.0625740\n",
      "Iteration: 31000, Train loss: 0.0625740, Test loss: 0.0625740, Best test loss: 0.0625740, Time: 1067.84\n",
      "New best model saved at iteration 32000 with test MSE: 0.0611825\n",
      "Iteration: 32000, Train loss: 0.0611825, Test loss: 0.0611825, Best test loss: 0.0611825, Time: 1102.99\n",
      "New best model saved at iteration 33000 with test MSE: 0.0599053\n",
      "Iteration: 33000, Train loss: 0.0599053, Test loss: 0.0599053, Best test loss: 0.0599053, Time: 1138.29\n",
      "New best model saved at iteration 34000 with test MSE: 0.0589751\n",
      "Iteration: 34000, Train loss: 0.0589751, Test loss: 0.0589751, Best test loss: 0.0589751, Time: 1173.20\n",
      "New best model saved at iteration 35000 with test MSE: 0.0580039\n",
      "Iteration: 35000, Train loss: 0.0580039, Test loss: 0.0580039, Best test loss: 0.0580039, Time: 1208.09\n",
      "New best model saved at iteration 36000 with test MSE: 0.0574662\n",
      "Iteration: 36000, Train loss: 0.0574662, Test loss: 0.0574662, Best test loss: 0.0574662, Time: 1243.27\n",
      "New best model saved at iteration 37000 with test MSE: 0.0552502\n",
      "Iteration: 37000, Train loss: 0.0552502, Test loss: 0.0552502, Best test loss: 0.0552502, Time: 1278.30\n",
      "New best model saved at iteration 38000 with test MSE: 0.0539215\n",
      "Iteration: 38000, Train loss: 0.0539215, Test loss: 0.0539215, Best test loss: 0.0539215, Time: 1313.56\n",
      "New best model saved at iteration 39000 with test MSE: 0.0522722\n",
      "Iteration: 39000, Train loss: 0.0522722, Test loss: 0.0522722, Best test loss: 0.0522722, Time: 1348.22\n",
      "New best model saved at iteration 40000 with test MSE: 0.0490190\n",
      "Iteration: 40000, Train loss: 0.0490190, Test loss: 0.0490190, Best test loss: 0.0490190, Time: 1383.26\n",
      "New best model saved at iteration 41000 with test MSE: 0.0474342\n",
      "Iteration: 41000, Train loss: 0.0474342, Test loss: 0.0474342, Best test loss: 0.0474342, Time: 1417.93\n",
      "New best model saved at iteration 42000 with test MSE: 0.0449396\n",
      "Iteration: 42000, Train loss: 0.0449396, Test loss: 0.0449396, Best test loss: 0.0449396, Time: 1452.20\n",
      "New best model saved at iteration 43000 with test MSE: 0.0438973\n",
      "Iteration: 43000, Train loss: 0.0438973, Test loss: 0.0438973, Best test loss: 0.0438973, Time: 1486.64\n",
      "New best model saved at iteration 44000 with test MSE: 0.0426552\n",
      "Iteration: 44000, Train loss: 0.0426552, Test loss: 0.0426552, Best test loss: 0.0426552, Time: 1520.70\n",
      "New best model saved at iteration 45000 with test MSE: 0.0412016\n",
      "Iteration: 45000, Train loss: 0.0412016, Test loss: 0.0412016, Best test loss: 0.0412016, Time: 1555.00\n",
      "New best model saved at iteration 46000 with test MSE: 0.0409029\n",
      "Iteration: 46000, Train loss: 0.0409029, Test loss: 0.0409029, Best test loss: 0.0409029, Time: 1589.45\n",
      "New best model saved at iteration 47000 with test MSE: 0.0403900\n",
      "Iteration: 47000, Train loss: 0.0403900, Test loss: 0.0403900, Best test loss: 0.0403900, Time: 1623.75\n",
      "New best model saved at iteration 48000 with test MSE: 0.0397312\n",
      "Iteration: 48000, Train loss: 0.0397312, Test loss: 0.0397312, Best test loss: 0.0397312, Time: 1658.42\n",
      "New best model saved at iteration 49000 with test MSE: 0.0386254\n",
      "Iteration: 49000, Train loss: 0.0386254, Test loss: 0.0386254, Best test loss: 0.0386254, Time: 1692.35\n",
      "New best model saved at iteration 50000 with test MSE: 0.0379477\n",
      "Iteration: 50000, Train loss: 0.0379477, Test loss: 0.0379477, Best test loss: 0.0379477, Time: 1726.66\n",
      "New best model saved at iteration 51000 with test MSE: 0.0369778\n",
      "Iteration: 51000, Train loss: 0.0369778, Test loss: 0.0369778, Best test loss: 0.0369778, Time: 1761.05\n",
      "New best model saved at iteration 52000 with test MSE: 0.0364286\n",
      "Iteration: 52000, Train loss: 0.0364286, Test loss: 0.0364286, Best test loss: 0.0364286, Time: 1795.99\n",
      "New best model saved at iteration 53000 with test MSE: 0.0362117\n",
      "Iteration: 53000, Train loss: 0.0362117, Test loss: 0.0362117, Best test loss: 0.0362117, Time: 1831.09\n",
      "New best model saved at iteration 54000 with test MSE: 0.0351535\n",
      "Iteration: 54000, Train loss: 0.0351535, Test loss: 0.0351535, Best test loss: 0.0351535, Time: 1865.54\n",
      "Iteration: 55000, Train loss: 0.0354693, Test loss: 0.0354693, Best test loss: 0.0351535, Time: 1900.50\n",
      "New best model saved at iteration 56000 with test MSE: 0.0341453\n",
      "Iteration: 56000, Train loss: 0.0341453, Test loss: 0.0341453, Best test loss: 0.0341453, Time: 1935.49\n",
      "New best model saved at iteration 57000 with test MSE: 0.0335340\n",
      "Iteration: 57000, Train loss: 0.0335340, Test loss: 0.0335340, Best test loss: 0.0335340, Time: 1970.30\n",
      "New best model saved at iteration 58000 with test MSE: 0.0327079\n",
      "Iteration: 58000, Train loss: 0.0327079, Test loss: 0.0327079, Best test loss: 0.0327079, Time: 2005.36\n",
      "New best model saved at iteration 59000 with test MSE: 0.0323372\n",
      "Iteration: 59000, Train loss: 0.0323372, Test loss: 0.0323372, Best test loss: 0.0323372, Time: 2039.84\n",
      "New best model saved at iteration 60000 with test MSE: 0.0323016\n",
      "Iteration: 60000, Train loss: 0.0323016, Test loss: 0.0323016, Best test loss: 0.0323016, Time: 2075.01\n",
      "New best model saved at iteration 61000 with test MSE: 0.0317382\n",
      "Iteration: 61000, Train loss: 0.0317382, Test loss: 0.0317382, Best test loss: 0.0317382, Time: 2110.14\n",
      "Iteration: 62000, Train loss: 0.0322670, Test loss: 0.0322670, Best test loss: 0.0317382, Time: 2145.13\n",
      "New best model saved at iteration 63000 with test MSE: 0.0307721\n",
      "Iteration: 63000, Train loss: 0.0307721, Test loss: 0.0307721, Best test loss: 0.0307721, Time: 2180.34\n",
      "New best model saved at iteration 64000 with test MSE: 0.0298747\n",
      "Iteration: 64000, Train loss: 0.0298747, Test loss: 0.0298747, Best test loss: 0.0298747, Time: 2214.76\n",
      "New best model saved at iteration 65000 with test MSE: 0.0292423\n",
      "Iteration: 65000, Train loss: 0.0292423, Test loss: 0.0292423, Best test loss: 0.0292423, Time: 2249.49\n",
      "New best model saved at iteration 66000 with test MSE: 0.0289415\n",
      "Iteration: 66000, Train loss: 0.0289415, Test loss: 0.0289415, Best test loss: 0.0289415, Time: 2284.53\n",
      "New best model saved at iteration 67000 with test MSE: 0.0286259\n",
      "Iteration: 67000, Train loss: 0.0286259, Test loss: 0.0286259, Best test loss: 0.0286259, Time: 2319.74\n",
      "New best model saved at iteration 68000 with test MSE: 0.0281222\n",
      "Iteration: 68000, Train loss: 0.0281222, Test loss: 0.0281222, Best test loss: 0.0281222, Time: 2355.01\n",
      "New best model saved at iteration 69000 with test MSE: 0.0276804\n",
      "Iteration: 69000, Train loss: 0.0276804, Test loss: 0.0276804, Best test loss: 0.0276804, Time: 2389.44\n",
      "New best model saved at iteration 70000 with test MSE: 0.0271189\n",
      "Iteration: 70000, Train loss: 0.0271189, Test loss: 0.0271189, Best test loss: 0.0271189, Time: 2424.54\n",
      "New best model saved at iteration 71000 with test MSE: 0.0266194\n",
      "Iteration: 71000, Train loss: 0.0266194, Test loss: 0.0266194, Best test loss: 0.0266194, Time: 2459.80\n",
      "New best model saved at iteration 72000 with test MSE: 0.0265350\n",
      "Iteration: 72000, Train loss: 0.0265350, Test loss: 0.0265350, Best test loss: 0.0265350, Time: 2494.88\n",
      "New best model saved at iteration 73000 with test MSE: 0.0257917\n",
      "Iteration: 73000, Train loss: 0.0257917, Test loss: 0.0257917, Best test loss: 0.0257917, Time: 2530.07\n",
      "Iteration: 74000, Train loss: 0.0262795, Test loss: 0.0262795, Best test loss: 0.0257917, Time: 2564.54\n",
      "New best model saved at iteration 75000 with test MSE: 0.0254369\n",
      "Iteration: 75000, Train loss: 0.0254369, Test loss: 0.0254369, Best test loss: 0.0254369, Time: 2599.75\n",
      "New best model saved at iteration 76000 with test MSE: 0.0252923\n",
      "Iteration: 76000, Train loss: 0.0252923, Test loss: 0.0252923, Best test loss: 0.0252923, Time: 2634.93\n",
      "New best model saved at iteration 77000 with test MSE: 0.0250564\n",
      "Iteration: 77000, Train loss: 0.0250564, Test loss: 0.0250564, Best test loss: 0.0250564, Time: 2669.90\n",
      "New best model saved at iteration 78000 with test MSE: 0.0244018\n",
      "Iteration: 78000, Train loss: 0.0244018, Test loss: 0.0244018, Best test loss: 0.0244018, Time: 2704.79\n",
      "New best model saved at iteration 79000 with test MSE: 0.0239311\n",
      "Iteration: 79000, Train loss: 0.0239311, Test loss: 0.0239311, Best test loss: 0.0239311, Time: 2739.39\n",
      "New best model saved at iteration 80000 with test MSE: 0.0237546\n",
      "Iteration: 80000, Train loss: 0.0237546, Test loss: 0.0237546, Best test loss: 0.0237546, Time: 2774.42\n",
      "Iteration: 81000, Train loss: 0.0238407, Test loss: 0.0238407, Best test loss: 0.0237546, Time: 2809.44\n",
      "New best model saved at iteration 82000 with test MSE: 0.0230194\n",
      "Iteration: 82000, Train loss: 0.0230194, Test loss: 0.0230194, Best test loss: 0.0230194, Time: 2844.56\n",
      "New best model saved at iteration 83000 with test MSE: 0.0224662\n",
      "Iteration: 83000, Train loss: 0.0224662, Test loss: 0.0224662, Best test loss: 0.0224662, Time: 2878.69\n",
      "Iteration: 84000, Train loss: 0.0225312, Test loss: 0.0225312, Best test loss: 0.0224662, Time: 2913.49\n",
      "New best model saved at iteration 85000 with test MSE: 0.0221712\n",
      "Iteration: 85000, Train loss: 0.0221712, Test loss: 0.0221712, Best test loss: 0.0221712, Time: 2948.46\n",
      "New best model saved at iteration 86000 with test MSE: 0.0214319\n",
      "Iteration: 86000, Train loss: 0.0214319, Test loss: 0.0214319, Best test loss: 0.0214319, Time: 2983.39\n",
      "New best model saved at iteration 87000 with test MSE: 0.0211418\n",
      "Iteration: 87000, Train loss: 0.0211418, Test loss: 0.0211418, Best test loss: 0.0211418, Time: 3018.50\n",
      "Iteration: 88000, Train loss: 0.0213746, Test loss: 0.0213746, Best test loss: 0.0211418, Time: 3052.82\n",
      "New best model saved at iteration 89000 with test MSE: 0.0206122\n",
      "Iteration: 89000, Train loss: 0.0206122, Test loss: 0.0206122, Best test loss: 0.0206122, Time: 3087.88\n",
      "New best model saved at iteration 90000 with test MSE: 0.0199218\n",
      "Iteration: 90000, Train loss: 0.0199218, Test loss: 0.0199218, Best test loss: 0.0199218, Time: 3122.84\n",
      "Iteration: 91000, Train loss: 0.0203592, Test loss: 0.0203592, Best test loss: 0.0199218, Time: 3157.80\n",
      "New best model saved at iteration 92000 with test MSE: 0.0197746\n",
      "Iteration: 92000, Train loss: 0.0197746, Test loss: 0.0197746, Best test loss: 0.0197746, Time: 3192.84\n",
      "New best model saved at iteration 93000 with test MSE: 0.0197033\n",
      "Iteration: 93000, Train loss: 0.0197033, Test loss: 0.0197033, Best test loss: 0.0197033, Time: 3227.03\n",
      "New best model saved at iteration 94000 with test MSE: 0.0194385\n",
      "Iteration: 94000, Train loss: 0.0194385, Test loss: 0.0194385, Best test loss: 0.0194385, Time: 3262.12\n",
      "New best model saved at iteration 95000 with test MSE: 0.0190638\n",
      "Iteration: 95000, Train loss: 0.0190638, Test loss: 0.0190638, Best test loss: 0.0190638, Time: 3297.22\n",
      "New best model saved at iteration 96000 with test MSE: 0.0183704\n",
      "Iteration: 96000, Train loss: 0.0183704, Test loss: 0.0183704, Best test loss: 0.0183704, Time: 3332.22\n",
      "Iteration: 97000, Train loss: 0.0185682, Test loss: 0.0185682, Best test loss: 0.0183704, Time: 3367.43\n",
      "Iteration: 98000, Train loss: 0.0183725, Test loss: 0.0183725, Best test loss: 0.0183704, Time: 3401.68\n",
      "New best model saved at iteration 99000 with test MSE: 0.0180921\n",
      "Iteration: 99000, Train loss: 0.0180921, Test loss: 0.0180921, Best test loss: 0.0180921, Time: 3436.68\n",
      "New best model saved at iteration 100000 with test MSE: 0.0177102\n",
      "Iteration: 100000, Train loss: 0.0177102, Test loss: 0.0177102, Best test loss: 0.0177102, Time: 3471.55\n",
      "New best model saved at iteration 101000 with test MSE: 0.0171859\n",
      "Iteration: 101000, Train loss: 0.0171859, Test loss: 0.0171859, Best test loss: 0.0171859, Time: 3506.59\n",
      "Iteration: 102000, Train loss: 0.0173275, Test loss: 0.0173275, Best test loss: 0.0171859, Time: 3541.63\n",
      "New best model saved at iteration 103000 with test MSE: 0.0169893\n",
      "Iteration: 103000, Train loss: 0.0169893, Test loss: 0.0169893, Best test loss: 0.0169893, Time: 3575.88\n",
      "New best model saved at iteration 104000 with test MSE: 0.0162384\n",
      "Iteration: 104000, Train loss: 0.0162384, Test loss: 0.0162384, Best test loss: 0.0162384, Time: 3610.93\n",
      "New best model saved at iteration 105000 with test MSE: 0.0159397\n",
      "Iteration: 105000, Train loss: 0.0159397, Test loss: 0.0159397, Best test loss: 0.0159397, Time: 3645.80\n",
      "New best model saved at iteration 106000 with test MSE: 0.0153124\n",
      "Iteration: 106000, Train loss: 0.0153124, Test loss: 0.0153124, Best test loss: 0.0153124, Time: 3680.26\n",
      "New best model saved at iteration 107000 with test MSE: 0.0152264\n",
      "Iteration: 107000, Train loss: 0.0152264, Test loss: 0.0152264, Best test loss: 0.0152264, Time: 3715.22\n",
      "New best model saved at iteration 108000 with test MSE: 0.0150389\n",
      "Iteration: 108000, Train loss: 0.0150389, Test loss: 0.0150389, Best test loss: 0.0150389, Time: 3749.37\n",
      "New best model saved at iteration 109000 with test MSE: 0.0146877\n",
      "Iteration: 109000, Train loss: 0.0146877, Test loss: 0.0146877, Best test loss: 0.0146877, Time: 3783.71\n",
      "New best model saved at iteration 110000 with test MSE: 0.0144815\n",
      "Iteration: 110000, Train loss: 0.0144815, Test loss: 0.0144815, Best test loss: 0.0144815, Time: 3818.00\n",
      "Iteration: 111000, Train loss: 0.0148652, Test loss: 0.0148652, Best test loss: 0.0144815, Time: 3852.37\n",
      "New best model saved at iteration 112000 with test MSE: 0.0139956\n",
      "Iteration: 112000, Train loss: 0.0139956, Test loss: 0.0139956, Best test loss: 0.0139956, Time: 3886.65\n",
      "Iteration: 113000, Train loss: 0.0146057, Test loss: 0.0146057, Best test loss: 0.0139956, Time: 3920.50\n",
      "Iteration: 114000, Train loss: 0.0142343, Test loss: 0.0142343, Best test loss: 0.0139956, Time: 3955.19\n",
      "New best model saved at iteration 115000 with test MSE: 0.0138979\n",
      "Iteration: 115000, Train loss: 0.0138979, Test loss: 0.0138979, Best test loss: 0.0138979, Time: 3989.60\n",
      "New best model saved at iteration 116000 with test MSE: 0.0133539\n",
      "Iteration: 116000, Train loss: 0.0133539, Test loss: 0.0133539, Best test loss: 0.0133539, Time: 4024.03\n",
      "Iteration: 117000, Train loss: 0.0139892, Test loss: 0.0139892, Best test loss: 0.0133539, Time: 4058.56\n",
      "Iteration: 118000, Train loss: 0.0133987, Test loss: 0.0133987, Best test loss: 0.0133539, Time: 4092.75\n",
      "Iteration: 119000, Train loss: 0.0134170, Test loss: 0.0134170, Best test loss: 0.0133539, Time: 4127.42\n",
      "New best model saved at iteration 120000 with test MSE: 0.0129441\n",
      "Iteration: 120000, Train loss: 0.0129441, Test loss: 0.0129441, Best test loss: 0.0129441, Time: 4162.24\n",
      "New best model saved at iteration 121000 with test MSE: 0.0129080\n",
      "Iteration: 121000, Train loss: 0.0129080, Test loss: 0.0129080, Best test loss: 0.0129080, Time: 4197.26\n",
      "New best model saved at iteration 122000 with test MSE: 0.0125877\n",
      "Iteration: 122000, Train loss: 0.0125877, Test loss: 0.0125877, Best test loss: 0.0125877, Time: 4232.44\n",
      "New best model saved at iteration 123000 with test MSE: 0.0125059\n",
      "Iteration: 123000, Train loss: 0.0125059, Test loss: 0.0125059, Best test loss: 0.0125059, Time: 4266.77\n",
      "Iteration: 124000, Train loss: 0.0126445, Test loss: 0.0126445, Best test loss: 0.0125059, Time: 4301.66\n",
      "Iteration: 125000, Train loss: 0.0131605, Test loss: 0.0131605, Best test loss: 0.0125059, Time: 4337.01\n",
      "New best model saved at iteration 126000 with test MSE: 0.0121424\n",
      "Iteration: 126000, Train loss: 0.0121424, Test loss: 0.0121424, Best test loss: 0.0121424, Time: 4371.96\n",
      "Iteration: 127000, Train loss: 0.0122863, Test loss: 0.0122863, Best test loss: 0.0121424, Time: 4407.00\n",
      "Iteration: 128000, Train loss: 0.0125661, Test loss: 0.0125661, Best test loss: 0.0121424, Time: 4441.17\n",
      "New best model saved at iteration 129000 with test MSE: 0.0120618\n",
      "Iteration: 129000, Train loss: 0.0120618, Test loss: 0.0120618, Best test loss: 0.0120618, Time: 4476.41\n",
      "Iteration: 130000, Train loss: 0.0122196, Test loss: 0.0122196, Best test loss: 0.0120618, Time: 4511.65\n",
      "New best model saved at iteration 131000 with test MSE: 0.0120475\n",
      "Iteration: 131000, Train loss: 0.0120475, Test loss: 0.0120475, Best test loss: 0.0120475, Time: 4546.51\n",
      "Iteration: 132000, Train loss: 0.0120818, Test loss: 0.0120818, Best test loss: 0.0120475, Time: 4581.58\n",
      "New best model saved at iteration 133000 with test MSE: 0.0118760\n",
      "Iteration: 133000, Train loss: 0.0118760, Test loss: 0.0118760, Best test loss: 0.0118760, Time: 4615.76\n",
      "New best model saved at iteration 134000 with test MSE: 0.0116704\n",
      "Iteration: 134000, Train loss: 0.0116704, Test loss: 0.0116704, Best test loss: 0.0116704, Time: 4650.77\n",
      "Iteration: 135000, Train loss: 0.0121553, Test loss: 0.0121553, Best test loss: 0.0116704, Time: 4685.29\n",
      "New best model saved at iteration 136000 with test MSE: 0.0115172\n",
      "Iteration: 136000, Train loss: 0.0115172, Test loss: 0.0115172, Best test loss: 0.0115172, Time: 4719.90\n",
      "New best model saved at iteration 137000 with test MSE: 0.0114482\n",
      "Iteration: 137000, Train loss: 0.0114482, Test loss: 0.0114482, Best test loss: 0.0114482, Time: 4754.99\n",
      "Iteration: 138000, Train loss: 0.0115053, Test loss: 0.0115053, Best test loss: 0.0114482, Time: 4789.14\n",
      "New best model saved at iteration 139000 with test MSE: 0.0113051\n",
      "Iteration: 139000, Train loss: 0.0113051, Test loss: 0.0113051, Best test loss: 0.0113051, Time: 4824.26\n",
      "New best model saved at iteration 140000 with test MSE: 0.0111479\n",
      "Iteration: 140000, Train loss: 0.0111479, Test loss: 0.0111479, Best test loss: 0.0111479, Time: 4858.80\n",
      "Iteration: 141000, Train loss: 0.0120788, Test loss: 0.0120788, Best test loss: 0.0111479, Time: 4893.21\n",
      "New best model saved at iteration 142000 with test MSE: 0.0108462\n",
      "Iteration: 142000, Train loss: 0.0108462, Test loss: 0.0108462, Best test loss: 0.0108462, Time: 4927.64\n",
      "Iteration: 143000, Train loss: 0.0109683, Test loss: 0.0109683, Best test loss: 0.0108462, Time: 4961.32\n",
      "Iteration: 144000, Train loss: 0.0110554, Test loss: 0.0110554, Best test loss: 0.0108462, Time: 4995.89\n",
      "Iteration: 145000, Train loss: 0.0110167, Test loss: 0.0110167, Best test loss: 0.0108462, Time: 5030.50\n",
      "New best model saved at iteration 146000 with test MSE: 0.0106529\n",
      "Iteration: 146000, Train loss: 0.0106529, Test loss: 0.0106529, Best test loss: 0.0106529, Time: 5064.70\n",
      "Iteration: 147000, Train loss: 0.0121584, Test loss: 0.0121584, Best test loss: 0.0106529, Time: 5098.98\n",
      "Iteration: 148000, Train loss: 0.0109140, Test loss: 0.0109140, Best test loss: 0.0106529, Time: 5132.69\n",
      "Iteration: 149000, Train loss: 0.0107175, Test loss: 0.0107175, Best test loss: 0.0106529, Time: 5167.21\n",
      "Iteration: 150000, Train loss: 0.0123771, Test loss: 0.0123771, Best test loss: 0.0106529, Time: 5201.44\n",
      "New best model saved at iteration 151000 with test MSE: 0.0104405\n",
      "Iteration: 151000, Train loss: 0.0104405, Test loss: 0.0104405, Best test loss: 0.0104405, Time: 5235.73\n",
      "Iteration: 152000, Train loss: 0.0105441, Test loss: 0.0105441, Best test loss: 0.0104405, Time: 5270.28\n",
      "New best model saved at iteration 153000 with test MSE: 0.0103721\n",
      "Iteration: 153000, Train loss: 0.0103721, Test loss: 0.0103721, Best test loss: 0.0103721, Time: 5304.96\n",
      "New best model saved at iteration 154000 with test MSE: 0.0103071\n",
      "Iteration: 154000, Train loss: 0.0103071, Test loss: 0.0103071, Best test loss: 0.0103071, Time: 5339.13\n",
      "Iteration: 155000, Train loss: 0.0105932, Test loss: 0.0105932, Best test loss: 0.0103071, Time: 5373.64\n",
      "Iteration: 156000, Train loss: 0.0108383, Test loss: 0.0108383, Best test loss: 0.0103071, Time: 5407.91\n",
      "Iteration: 157000, Train loss: 0.0103415, Test loss: 0.0103415, Best test loss: 0.0103071, Time: 5442.12\n",
      "Iteration: 158000, Train loss: 0.0104248, Test loss: 0.0104248, Best test loss: 0.0103071, Time: 5476.51\n",
      "New best model saved at iteration 159000 with test MSE: 0.0098108\n",
      "Iteration: 159000, Train loss: 0.0098108, Test loss: 0.0098108, Best test loss: 0.0098108, Time: 5510.22\n",
      "Iteration: 160000, Train loss: 0.0101815, Test loss: 0.0101815, Best test loss: 0.0098108, Time: 5544.57\n",
      "Iteration: 161000, Train loss: 0.0102298, Test loss: 0.0102298, Best test loss: 0.0098108, Time: 5578.95\n",
      "Iteration: 162000, Train loss: 0.0104940, Test loss: 0.0104940, Best test loss: 0.0098108, Time: 5613.27\n",
      "Iteration: 163000, Train loss: 0.0100243, Test loss: 0.0100243, Best test loss: 0.0098108, Time: 5647.81\n",
      "Iteration: 164000, Train loss: 0.0099340, Test loss: 0.0099340, Best test loss: 0.0098108, Time: 5681.56\n",
      "Iteration: 165000, Train loss: 0.0099526, Test loss: 0.0099526, Best test loss: 0.0098108, Time: 5716.90\n",
      "Iteration: 166000, Train loss: 0.0098996, Test loss: 0.0098996, Best test loss: 0.0098108, Time: 5752.02\n",
      "New best model saved at iteration 167000 with test MSE: 0.0097199\n",
      "Iteration: 167000, Train loss: 0.0097199, Test loss: 0.0097199, Best test loss: 0.0097199, Time: 5787.35\n",
      "Iteration: 168000, Train loss: 0.0099262, Test loss: 0.0099262, Best test loss: 0.0097199, Time: 5821.55\n",
      "New best model saved at iteration 169000 with test MSE: 0.0096337\n",
      "Iteration: 169000, Train loss: 0.0096337, Test loss: 0.0096337, Best test loss: 0.0096337, Time: 5856.65\n",
      "Iteration: 170000, Train loss: 0.0102603, Test loss: 0.0102603, Best test loss: 0.0096337, Time: 5891.82\n",
      "Iteration: 171000, Train loss: 0.0097637, Test loss: 0.0097637, Best test loss: 0.0096337, Time: 5927.18\n",
      "Iteration: 172000, Train loss: 0.0097379, Test loss: 0.0097379, Best test loss: 0.0096337, Time: 5962.45\n",
      "New best model saved at iteration 173000 with test MSE: 0.0095534\n",
      "Iteration: 173000, Train loss: 0.0095534, Test loss: 0.0095534, Best test loss: 0.0095534, Time: 5996.70\n",
      "Iteration: 174000, Train loss: 0.0096601, Test loss: 0.0096601, Best test loss: 0.0095534, Time: 6031.88\n",
      "New best model saved at iteration 175000 with test MSE: 0.0095161\n",
      "Iteration: 175000, Train loss: 0.0095161, Test loss: 0.0095161, Best test loss: 0.0095161, Time: 6067.14\n",
      "Iteration: 176000, Train loss: 0.0096473, Test loss: 0.0096473, Best test loss: 0.0095161, Time: 6102.27\n",
      "New best model saved at iteration 177000 with test MSE: 0.0094414\n",
      "Iteration: 177000, Train loss: 0.0094414, Test loss: 0.0094414, Best test loss: 0.0094414, Time: 6136.47\n",
      "Iteration: 178000, Train loss: 0.0094858, Test loss: 0.0094858, Best test loss: 0.0094414, Time: 6171.73\n",
      "New best model saved at iteration 179000 with test MSE: 0.0093357\n",
      "Iteration: 179000, Train loss: 0.0093357, Test loss: 0.0093357, Best test loss: 0.0093357, Time: 6206.21\n",
      "Iteration: 180000, Train loss: 0.0094556, Test loss: 0.0094556, Best test loss: 0.0093357, Time: 6240.60\n",
      "New best model saved at iteration 181000 with test MSE: 0.0093096\n",
      "Iteration: 181000, Train loss: 0.0093096, Test loss: 0.0093096, Best test loss: 0.0093096, Time: 6275.04\n",
      "Iteration: 182000, Train loss: 0.0093745, Test loss: 0.0093745, Best test loss: 0.0093096, Time: 6308.67\n",
      "New best model saved at iteration 183000 with test MSE: 0.0091105\n",
      "Iteration: 183000, Train loss: 0.0091105, Test loss: 0.0091105, Best test loss: 0.0091105, Time: 6343.12\n",
      "Iteration: 184000, Train loss: 0.0094179, Test loss: 0.0094179, Best test loss: 0.0091105, Time: 6377.54\n",
      "Iteration: 185000, Train loss: 0.0091555, Test loss: 0.0091555, Best test loss: 0.0091105, Time: 6411.94\n",
      "Iteration: 186000, Train loss: 0.0092404, Test loss: 0.0092404, Best test loss: 0.0091105, Time: 6446.14\n",
      "Iteration: 187000, Train loss: 0.0096137, Test loss: 0.0096137, Best test loss: 0.0091105, Time: 6479.96\n",
      "Iteration: 188000, Train loss: 0.0094261, Test loss: 0.0094261, Best test loss: 0.0091105, Time: 6514.34\n",
      "New best model saved at iteration 189000 with test MSE: 0.0091026\n",
      "Iteration: 189000, Train loss: 0.0091026, Test loss: 0.0091026, Best test loss: 0.0091026, Time: 6548.64\n",
      "New best model saved at iteration 190000 with test MSE: 0.0090556\n",
      "Iteration: 190000, Train loss: 0.0090556, Test loss: 0.0090556, Best test loss: 0.0090556, Time: 6583.15\n",
      "Iteration: 191000, Train loss: 0.0091407, Test loss: 0.0091407, Best test loss: 0.0090556, Time: 6617.17\n",
      "New best model saved at iteration 192000 with test MSE: 0.0089001\n",
      "Iteration: 192000, Train loss: 0.0089001, Test loss: 0.0089001, Best test loss: 0.0089001, Time: 6651.50\n",
      "Iteration: 193000, Train loss: 0.0090188, Test loss: 0.0090188, Best test loss: 0.0089001, Time: 6686.00\n",
      "Iteration: 194000, Train loss: 0.0092068, Test loss: 0.0092068, Best test loss: 0.0089001, Time: 6720.42\n",
      "New best model saved at iteration 195000 with test MSE: 0.0088418\n",
      "Iteration: 195000, Train loss: 0.0088418, Test loss: 0.0088418, Best test loss: 0.0088418, Time: 6754.85\n",
      "Iteration: 196000, Train loss: 0.0088535, Test loss: 0.0088535, Best test loss: 0.0088418, Time: 6788.59\n",
      "Iteration: 197000, Train loss: 0.0090071, Test loss: 0.0090071, Best test loss: 0.0088418, Time: 6822.99\n",
      "Iteration: 198000, Train loss: 0.0088587, Test loss: 0.0088587, Best test loss: 0.0088418, Time: 6857.44\n",
      "New best model saved at iteration 199000 with test MSE: 0.0084936\n",
      "Iteration: 199000, Train loss: 0.0084936, Test loss: 0.0084936, Best test loss: 0.0084936, Time: 6891.71\n",
      "Iteration: 200000, Train loss: 0.0087805, Test loss: 0.0087805, Best test loss: 0.0084936, Time: 6925.99\n",
      "Iteration: 201000, Train loss: 0.0091434, Test loss: 0.0091434, Best test loss: 0.0084936, Time: 6959.60\n",
      "Iteration: 202000, Train loss: 0.0088874, Test loss: 0.0088874, Best test loss: 0.0084936, Time: 6994.07\n",
      "Iteration: 203000, Train loss: 0.0090123, Test loss: 0.0090123, Best test loss: 0.0084936, Time: 7028.58\n",
      "Iteration: 204000, Train loss: 0.0087171, Test loss: 0.0087171, Best test loss: 0.0084936, Time: 7062.81\n",
      "Iteration: 205000, Train loss: 0.0087235, Test loss: 0.0087235, Best test loss: 0.0084936, Time: 7097.21\n",
      "Iteration: 206000, Train loss: 0.0086252, Test loss: 0.0086252, Best test loss: 0.0084936, Time: 7130.86\n",
      "Iteration: 207000, Train loss: 0.0092827, Test loss: 0.0092827, Best test loss: 0.0084936, Time: 7165.31\n",
      "Iteration: 208000, Train loss: 0.0089144, Test loss: 0.0089144, Best test loss: 0.0084936, Time: 7199.73\n",
      "Iteration: 209000, Train loss: 0.0085614, Test loss: 0.0085614, Best test loss: 0.0084936, Time: 7234.70\n",
      "Iteration: 210000, Train loss: 0.0085137, Test loss: 0.0085137, Best test loss: 0.0084936, Time: 7270.06\n",
      "Iteration: 211000, Train loss: 0.0087210, Test loss: 0.0087210, Best test loss: 0.0084936, Time: 7303.06\n",
      "Iteration: 212000, Train loss: 0.0087968, Test loss: 0.0087968, Best test loss: 0.0084936, Time: 7336.42\n",
      "Iteration: 213000, Train loss: 0.0085156, Test loss: 0.0085156, Best test loss: 0.0084936, Time: 7369.72\n",
      "New best model saved at iteration 214000 with test MSE: 0.0083572\n",
      "Iteration: 214000, Train loss: 0.0083572, Test loss: 0.0083572, Best test loss: 0.0083572, Time: 7403.09\n",
      "Iteration: 215000, Train loss: 0.0085901, Test loss: 0.0085901, Best test loss: 0.0083572, Time: 7436.26\n",
      "Iteration: 216000, Train loss: 0.0084727, Test loss: 0.0084727, Best test loss: 0.0083572, Time: 7469.18\n",
      "New best model saved at iteration 217000 with test MSE: 0.0082489\n",
      "Iteration: 217000, Train loss: 0.0082489, Test loss: 0.0082489, Best test loss: 0.0082489, Time: 7502.02\n",
      "Iteration: 218000, Train loss: 0.0086494, Test loss: 0.0086494, Best test loss: 0.0082489, Time: 7534.91\n",
      "Iteration: 219000, Train loss: 0.0084946, Test loss: 0.0084946, Best test loss: 0.0082489, Time: 7567.89\n",
      "Iteration: 220000, Train loss: 0.0087127, Test loss: 0.0087127, Best test loss: 0.0082489, Time: 7600.74\n",
      "Iteration: 221000, Train loss: 0.0084614, Test loss: 0.0084614, Best test loss: 0.0082489, Time: 7633.63\n",
      "Iteration: 222000, Train loss: 0.0084354, Test loss: 0.0084354, Best test loss: 0.0082489, Time: 7666.46\n",
      "New best model saved at iteration 223000 with test MSE: 0.0082171\n",
      "Iteration: 223000, Train loss: 0.0082171, Test loss: 0.0082171, Best test loss: 0.0082171, Time: 7699.39\n",
      "Iteration: 224000, Train loss: 0.0083888, Test loss: 0.0083888, Best test loss: 0.0082171, Time: 7732.30\n",
      "Iteration: 225000, Train loss: 0.0083991, Test loss: 0.0083991, Best test loss: 0.0082171, Time: 7765.24\n",
      "Iteration: 226000, Train loss: 0.0084816, Test loss: 0.0084816, Best test loss: 0.0082171, Time: 7798.01\n",
      "Iteration: 227000, Train loss: 0.0083152, Test loss: 0.0083152, Best test loss: 0.0082171, Time: 7830.76\n",
      "Iteration: 228000, Train loss: 0.0084958, Test loss: 0.0084958, Best test loss: 0.0082171, Time: 7863.57\n",
      "Iteration: 229000, Train loss: 0.0084416, Test loss: 0.0084416, Best test loss: 0.0082171, Time: 7896.31\n",
      "New best model saved at iteration 230000 with test MSE: 0.0080276\n",
      "Iteration: 230000, Train loss: 0.0080276, Test loss: 0.0080276, Best test loss: 0.0080276, Time: 7929.14\n",
      "Iteration: 231000, Train loss: 0.0083473, Test loss: 0.0083473, Best test loss: 0.0080276, Time: 7961.53\n",
      "Iteration: 232000, Train loss: 0.0087699, Test loss: 0.0087699, Best test loss: 0.0080276, Time: 7994.40\n",
      "Iteration: 233000, Train loss: 0.0082165, Test loss: 0.0082165, Best test loss: 0.0080276, Time: 8027.23\n",
      "New best model saved at iteration 234000 with test MSE: 0.0080144\n",
      "Iteration: 234000, Train loss: 0.0080144, Test loss: 0.0080144, Best test loss: 0.0080144, Time: 8060.11\n",
      "Iteration: 235000, Train loss: 0.0082523, Test loss: 0.0082523, Best test loss: 0.0080144, Time: 8092.71\n",
      "Iteration: 236000, Train loss: 0.0081031, Test loss: 0.0081031, Best test loss: 0.0080144, Time: 8125.59\n",
      "New best model saved at iteration 237000 with test MSE: 0.0079969\n",
      "Iteration: 237000, Train loss: 0.0079969, Test loss: 0.0079969, Best test loss: 0.0079969, Time: 8158.47\n",
      "Iteration: 238000, Train loss: 0.0080143, Test loss: 0.0080143, Best test loss: 0.0079969, Time: 8190.94\n",
      "Iteration: 239000, Train loss: 0.0081000, Test loss: 0.0081000, Best test loss: 0.0079969, Time: 8223.78\n",
      "Iteration: 240000, Train loss: 0.0080233, Test loss: 0.0080233, Best test loss: 0.0079969, Time: 8256.57\n",
      "Iteration: 241000, Train loss: 0.0081047, Test loss: 0.0081047, Best test loss: 0.0079969, Time: 8289.50\n",
      "Iteration: 242000, Train loss: 0.0081170, Test loss: 0.0081170, Best test loss: 0.0079969, Time: 8322.29\n",
      "Iteration: 243000, Train loss: 0.0081914, Test loss: 0.0081914, Best test loss: 0.0079969, Time: 8355.13\n",
      "Iteration: 244000, Train loss: 0.0080329, Test loss: 0.0080329, Best test loss: 0.0079969, Time: 8388.04\n",
      "New best model saved at iteration 245000 with test MSE: 0.0079691\n",
      "Iteration: 245000, Train loss: 0.0079691, Test loss: 0.0079691, Best test loss: 0.0079691, Time: 8421.08\n",
      "New best model saved at iteration 246000 with test MSE: 0.0079149\n",
      "Iteration: 246000, Train loss: 0.0079149, Test loss: 0.0079149, Best test loss: 0.0079149, Time: 8453.50\n",
      "New best model saved at iteration 247000 with test MSE: 0.0078828\n",
      "Iteration: 247000, Train loss: 0.0078828, Test loss: 0.0078828, Best test loss: 0.0078828, Time: 8486.06\n",
      "Iteration: 248000, Train loss: 0.0079121, Test loss: 0.0079121, Best test loss: 0.0078828, Time: 8518.55\n",
      "New best model saved at iteration 249000 with test MSE: 0.0078022\n",
      "Iteration: 249000, Train loss: 0.0078022, Test loss: 0.0078022, Best test loss: 0.0078022, Time: 8551.39\n",
      "Iteration: 250000, Train loss: 0.0080865, Test loss: 0.0080865, Best test loss: 0.0078022, Time: 8583.87\n",
      "Iteration: 251000, Train loss: 0.0095480, Test loss: 0.0095480, Best test loss: 0.0078022, Time: 8616.70\n",
      "Iteration: 252000, Train loss: 0.0078818, Test loss: 0.0078818, Best test loss: 0.0078022, Time: 8649.60\n",
      "Iteration: 253000, Train loss: 0.0078492, Test loss: 0.0078492, Best test loss: 0.0078022, Time: 8682.42\n",
      "New best model saved at iteration 254000 with test MSE: 0.0077631\n",
      "Iteration: 254000, Train loss: 0.0077631, Test loss: 0.0077631, Best test loss: 0.0077631, Time: 8715.28\n",
      "New best model saved at iteration 255000 with test MSE: 0.0076973\n",
      "Iteration: 255000, Train loss: 0.0076973, Test loss: 0.0076973, Best test loss: 0.0076973, Time: 8747.78\n",
      "Iteration: 256000, Train loss: 0.0078132, Test loss: 0.0078132, Best test loss: 0.0076973, Time: 8780.31\n",
      "New best model saved at iteration 257000 with test MSE: 0.0076814\n",
      "Iteration: 257000, Train loss: 0.0076814, Test loss: 0.0076814, Best test loss: 0.0076814, Time: 8813.20\n",
      "Iteration: 258000, Train loss: 0.0078598, Test loss: 0.0078598, Best test loss: 0.0076814, Time: 8845.76\n",
      "Iteration: 259000, Train loss: 0.0077577, Test loss: 0.0077577, Best test loss: 0.0076814, Time: 8878.57\n",
      "New best model saved at iteration 260000 with test MSE: 0.0075462\n",
      "Iteration: 260000, Train loss: 0.0075462, Test loss: 0.0075462, Best test loss: 0.0075462, Time: 8911.33\n",
      "Iteration: 261000, Train loss: 0.0077914, Test loss: 0.0077914, Best test loss: 0.0075462, Time: 8943.84\n",
      "Iteration: 262000, Train loss: 0.0075891, Test loss: 0.0075891, Best test loss: 0.0075462, Time: 8976.67\n",
      "Iteration: 263000, Train loss: 0.0076637, Test loss: 0.0076637, Best test loss: 0.0075462, Time: 9009.51\n",
      "Iteration: 264000, Train loss: 0.0078162, Test loss: 0.0078162, Best test loss: 0.0075462, Time: 9042.32\n",
      "New best model saved at iteration 265000 with test MSE: 0.0075447\n",
      "Iteration: 265000, Train loss: 0.0075447, Test loss: 0.0075447, Best test loss: 0.0075447, Time: 9075.22\n",
      "Iteration: 266000, Train loss: 0.0087239, Test loss: 0.0087239, Best test loss: 0.0075447, Time: 9107.78\n",
      "Iteration: 267000, Train loss: 0.0075756, Test loss: 0.0075756, Best test loss: 0.0075447, Time: 9140.59\n",
      "Iteration: 268000, Train loss: 0.0078000, Test loss: 0.0078000, Best test loss: 0.0075447, Time: 9173.49\n",
      "Iteration: 269000, Train loss: 0.0076683, Test loss: 0.0076683, Best test loss: 0.0075447, Time: 9206.35\n",
      "New best model saved at iteration 270000 with test MSE: 0.0075413\n",
      "Iteration: 270000, Train loss: 0.0075413, Test loss: 0.0075413, Best test loss: 0.0075413, Time: 9239.20\n",
      "Iteration: 271000, Train loss: 0.0076009, Test loss: 0.0076009, Best test loss: 0.0075413, Time: 9271.72\n",
      "Iteration: 272000, Train loss: 0.0075675, Test loss: 0.0075675, Best test loss: 0.0075413, Time: 9304.50\n",
      "Iteration: 273000, Train loss: 0.0076136, Test loss: 0.0076136, Best test loss: 0.0075413, Time: 9337.56\n",
      "Iteration: 274000, Train loss: 0.0076383, Test loss: 0.0076383, Best test loss: 0.0075413, Time: 9370.32\n",
      "New best model saved at iteration 275000 with test MSE: 0.0073008\n",
      "Iteration: 275000, Train loss: 0.0073008, Test loss: 0.0073008, Best test loss: 0.0073008, Time: 9403.11\n",
      "Iteration: 276000, Train loss: 0.0077686, Test loss: 0.0077686, Best test loss: 0.0073008, Time: 9435.51\n",
      "Iteration: 277000, Train loss: 0.0074763, Test loss: 0.0074763, Best test loss: 0.0073008, Time: 9468.29\n",
      "Iteration: 278000, Train loss: 0.0077448, Test loss: 0.0077448, Best test loss: 0.0073008, Time: 9501.00\n",
      "Iteration: 279000, Train loss: 0.0074853, Test loss: 0.0074853, Best test loss: 0.0073008, Time: 9533.78\n",
      "Iteration: 280000, Train loss: 0.0077116, Test loss: 0.0077116, Best test loss: 0.0073008, Time: 9566.46\n",
      "Iteration: 281000, Train loss: 0.0075357, Test loss: 0.0075357, Best test loss: 0.0073008, Time: 9599.19\n",
      "Iteration: 282000, Train loss: 0.0075190, Test loss: 0.0075190, Best test loss: 0.0073008, Time: 9631.90\n",
      "Iteration: 283000, Train loss: 0.0076184, Test loss: 0.0076184, Best test loss: 0.0073008, Time: 9664.64\n",
      "Iteration: 284000, Train loss: 0.0074131, Test loss: 0.0074131, Best test loss: 0.0073008, Time: 9697.32\n",
      "Iteration: 285000, Train loss: 0.0074746, Test loss: 0.0074746, Best test loss: 0.0073008, Time: 9730.01\n",
      "Iteration: 286000, Train loss: 0.0073860, Test loss: 0.0073860, Best test loss: 0.0073008, Time: 9762.76\n",
      "New best model saved at iteration 287000 with test MSE: 0.0072514\n",
      "Iteration: 287000, Train loss: 0.0072514, Test loss: 0.0072514, Best test loss: 0.0072514, Time: 9795.42\n",
      "Iteration: 288000, Train loss: 0.0074622, Test loss: 0.0074622, Best test loss: 0.0072514, Time: 9827.77\n",
      "Iteration: 289000, Train loss: 0.0073188, Test loss: 0.0073188, Best test loss: 0.0072514, Time: 9860.04\n",
      "Iteration: 290000, Train loss: 0.0073919, Test loss: 0.0073919, Best test loss: 0.0072514, Time: 9892.27\n",
      "Iteration: 291000, Train loss: 0.0074198, Test loss: 0.0074198, Best test loss: 0.0072514, Time: 9924.44\n",
      "Iteration: 292000, Train loss: 0.0074843, Test loss: 0.0074843, Best test loss: 0.0072514, Time: 9955.69\n",
      "New best model saved at iteration 293000 with test MSE: 0.0072503\n",
      "Iteration: 293000, Train loss: 0.0072503, Test loss: 0.0072503, Best test loss: 0.0072503, Time: 9985.59\n",
      "Iteration: 294000, Train loss: 0.0073242, Test loss: 0.0073242, Best test loss: 0.0072503, Time: 10015.54\n",
      "Iteration: 295000, Train loss: 0.0077648, Test loss: 0.0077648, Best test loss: 0.0072503, Time: 10045.41\n",
      "Iteration: 296000, Train loss: 0.0073085, Test loss: 0.0073085, Best test loss: 0.0072503, Time: 10075.19\n",
      "Iteration: 297000, Train loss: 0.0073285, Test loss: 0.0073285, Best test loss: 0.0072503, Time: 10104.94\n",
      "Iteration: 298000, Train loss: 0.0074657, Test loss: 0.0074657, Best test loss: 0.0072503, Time: 10134.73\n",
      "New best model saved at iteration 299000 with test MSE: 0.0071937\n",
      "Iteration: 299000, Train loss: 0.0071937, Test loss: 0.0071937, Best test loss: 0.0071937, Time: 10164.73\n",
      "Iteration: 300000, Train loss: 0.0075120, Test loss: 0.0075120, Best test loss: 0.0071937, Time: 10194.67\n",
      "Iteration: 301000, Train loss: 0.0073636, Test loss: 0.0073636, Best test loss: 0.0071937, Time: 10225.24\n",
      "New best model saved at iteration 302000 with test MSE: 0.0071781\n",
      "Iteration: 302000, Train loss: 0.0071781, Test loss: 0.0071781, Best test loss: 0.0071781, Time: 10258.05\n",
      "Iteration: 303000, Train loss: 0.0072100, Test loss: 0.0072100, Best test loss: 0.0071781, Time: 10290.97\n",
      "Iteration: 304000, Train loss: 0.0077321, Test loss: 0.0077321, Best test loss: 0.0071781, Time: 10323.63\n",
      "Iteration: 305000, Train loss: 0.0074137, Test loss: 0.0074137, Best test loss: 0.0071781, Time: 10356.70\n",
      "Iteration: 306000, Train loss: 0.0071995, Test loss: 0.0071995, Best test loss: 0.0071781, Time: 10389.47\n",
      "New best model saved at iteration 307000 with test MSE: 0.0071083\n",
      "Iteration: 307000, Train loss: 0.0071083, Test loss: 0.0071083, Best test loss: 0.0071083, Time: 10422.39\n",
      "New best model saved at iteration 308000 with test MSE: 0.0069941\n",
      "Iteration: 308000, Train loss: 0.0069941, Test loss: 0.0069941, Best test loss: 0.0069941, Time: 10455.95\n",
      "Iteration: 309000, Train loss: 0.0072798, Test loss: 0.0072798, Best test loss: 0.0069941, Time: 10489.03\n",
      "Iteration: 310000, Train loss: 0.0071522, Test loss: 0.0071522, Best test loss: 0.0069941, Time: 10522.07\n",
      "Iteration: 311000, Train loss: 0.0074646, Test loss: 0.0074646, Best test loss: 0.0069941, Time: 10555.24\n",
      "New best model saved at iteration 312000 with test MSE: 0.0069208\n",
      "Iteration: 312000, Train loss: 0.0069208, Test loss: 0.0069208, Best test loss: 0.0069208, Time: 10588.42\n",
      "Iteration: 313000, Train loss: 0.0070893, Test loss: 0.0070893, Best test loss: 0.0069208, Time: 10621.60\n",
      "Iteration: 314000, Train loss: 0.0071741, Test loss: 0.0071741, Best test loss: 0.0069208, Time: 10654.66\n",
      "Iteration: 315000, Train loss: 0.0071346, Test loss: 0.0071346, Best test loss: 0.0069208, Time: 10687.88\n",
      "Iteration: 316000, Train loss: 0.0070737, Test loss: 0.0070737, Best test loss: 0.0069208, Time: 10719.15\n",
      "Iteration: 317000, Train loss: 0.0071365, Test loss: 0.0071365, Best test loss: 0.0069208, Time: 10749.41\n",
      "New best model saved at iteration 318000 with test MSE: 0.0069171\n",
      "Iteration: 318000, Train loss: 0.0069171, Test loss: 0.0069171, Best test loss: 0.0069171, Time: 10779.65\n",
      "Iteration: 319000, Train loss: 0.0074343, Test loss: 0.0074343, Best test loss: 0.0069171, Time: 10809.81\n",
      "Iteration: 320000, Train loss: 0.0071419, Test loss: 0.0071419, Best test loss: 0.0069171, Time: 10840.02\n",
      "Iteration: 321000, Train loss: 0.0071869, Test loss: 0.0071869, Best test loss: 0.0069171, Time: 10870.14\n",
      "Iteration: 322000, Train loss: 0.0073178, Test loss: 0.0073178, Best test loss: 0.0069171, Time: 10902.55\n",
      "Iteration: 323000, Train loss: 0.0070164, Test loss: 0.0070164, Best test loss: 0.0069171, Time: 10935.15\n",
      "Iteration: 324000, Train loss: 0.0071655, Test loss: 0.0071655, Best test loss: 0.0069171, Time: 10967.82\n",
      "Iteration: 325000, Train loss: 0.0072910, Test loss: 0.0072910, Best test loss: 0.0069171, Time: 11000.61\n",
      "New best model saved at iteration 326000 with test MSE: 0.0069100\n",
      "Iteration: 326000, Train loss: 0.0069100, Test loss: 0.0069100, Best test loss: 0.0069100, Time: 11033.28\n",
      "Iteration: 327000, Train loss: 0.0070203, Test loss: 0.0070203, Best test loss: 0.0069100, Time: 11065.92\n",
      "Iteration: 328000, Train loss: 0.0069129, Test loss: 0.0069129, Best test loss: 0.0069100, Time: 11098.52\n",
      "Iteration: 329000, Train loss: 0.0069312, Test loss: 0.0069312, Best test loss: 0.0069100, Time: 11131.18\n",
      "Iteration: 330000, Train loss: 0.0069145, Test loss: 0.0069145, Best test loss: 0.0069100, Time: 11163.76\n",
      "New best model saved at iteration 331000 with test MSE: 0.0067715\n",
      "Iteration: 331000, Train loss: 0.0067715, Test loss: 0.0067715, Best test loss: 0.0067715, Time: 11196.48\n",
      "New best model saved at iteration 332000 with test MSE: 0.0067165\n",
      "Iteration: 332000, Train loss: 0.0067165, Test loss: 0.0067165, Best test loss: 0.0067165, Time: 11229.30\n",
      "Iteration: 333000, Train loss: 0.0067338, Test loss: 0.0067338, Best test loss: 0.0067165, Time: 11262.09\n",
      "Iteration: 334000, Train loss: 0.0069488, Test loss: 0.0069488, Best test loss: 0.0067165, Time: 11294.90\n",
      "Iteration: 335000, Train loss: 0.0068246, Test loss: 0.0068246, Best test loss: 0.0067165, Time: 11327.61\n",
      "Iteration: 336000, Train loss: 0.0070998, Test loss: 0.0070998, Best test loss: 0.0067165, Time: 11360.36\n",
      "Iteration: 337000, Train loss: 0.0069871, Test loss: 0.0069871, Best test loss: 0.0067165, Time: 11393.00\n",
      "Iteration: 338000, Train loss: 0.0067373, Test loss: 0.0067373, Best test loss: 0.0067165, Time: 11425.76\n",
      "Iteration: 339000, Train loss: 0.0072213, Test loss: 0.0072213, Best test loss: 0.0067165, Time: 11458.49\n",
      "Iteration: 340000, Train loss: 0.0068402, Test loss: 0.0068402, Best test loss: 0.0067165, Time: 11491.54\n",
      "Iteration: 341000, Train loss: 0.0068421, Test loss: 0.0068421, Best test loss: 0.0067165, Time: 11524.82\n",
      "Iteration: 342000, Train loss: 0.0067281, Test loss: 0.0067281, Best test loss: 0.0067165, Time: 11555.58\n",
      "Iteration: 343000, Train loss: 0.0068233, Test loss: 0.0068233, Best test loss: 0.0067165, Time: 11585.71\n",
      "Iteration: 344000, Train loss: 0.0068058, Test loss: 0.0068058, Best test loss: 0.0067165, Time: 11615.80\n",
      "Iteration: 345000, Train loss: 0.0067801, Test loss: 0.0067801, Best test loss: 0.0067165, Time: 11645.97\n",
      "Iteration: 346000, Train loss: 0.0067417, Test loss: 0.0067417, Best test loss: 0.0067165, Time: 11676.08\n",
      "New best model saved at iteration 347000 with test MSE: 0.0066556\n",
      "Iteration: 347000, Train loss: 0.0066556, Test loss: 0.0066556, Best test loss: 0.0066556, Time: 11706.21\n",
      "Iteration: 348000, Train loss: 0.0068826, Test loss: 0.0068826, Best test loss: 0.0066556, Time: 11739.02\n",
      "Iteration: 349000, Train loss: 0.0069183, Test loss: 0.0069183, Best test loss: 0.0066556, Time: 11771.82\n",
      "New best model saved at iteration 350000 with test MSE: 0.0066491\n",
      "Iteration: 350000, Train loss: 0.0066491, Test loss: 0.0066491, Best test loss: 0.0066491, Time: 11804.63\n",
      "Iteration: 351000, Train loss: 0.0067335, Test loss: 0.0067335, Best test loss: 0.0066491, Time: 11837.36\n",
      "Iteration: 352000, Train loss: 0.0068221, Test loss: 0.0068221, Best test loss: 0.0066491, Time: 11870.01\n",
      "Iteration: 353000, Train loss: 0.0067047, Test loss: 0.0067047, Best test loss: 0.0066491, Time: 11901.96\n",
      "Iteration: 354000, Train loss: 0.0070636, Test loss: 0.0070636, Best test loss: 0.0066491, Time: 11932.04\n",
      "Iteration: 355000, Train loss: 0.0067299, Test loss: 0.0067299, Best test loss: 0.0066491, Time: 11962.22\n",
      "Iteration: 356000, Train loss: 0.0068631, Test loss: 0.0068631, Best test loss: 0.0066491, Time: 11992.29\n",
      "Iteration: 357000, Train loss: 0.0067261, Test loss: 0.0067261, Best test loss: 0.0066491, Time: 12022.49\n",
      "New best model saved at iteration 358000 with test MSE: 0.0066391\n",
      "Iteration: 358000, Train loss: 0.0066391, Test loss: 0.0066391, Best test loss: 0.0066391, Time: 12052.56\n",
      "Iteration: 359000, Train loss: 0.0071715, Test loss: 0.0071715, Best test loss: 0.0066391, Time: 12082.52\n",
      "Iteration: 360000, Train loss: 0.0068508, Test loss: 0.0068508, Best test loss: 0.0066391, Time: 12112.39\n",
      "Iteration: 361000, Train loss: 0.0067043, Test loss: 0.0067043, Best test loss: 0.0066391, Time: 12142.31\n",
      "Iteration: 362000, Train loss: 0.0068903, Test loss: 0.0068903, Best test loss: 0.0066391, Time: 12172.50\n",
      "Iteration: 363000, Train loss: 0.0067646, Test loss: 0.0067646, Best test loss: 0.0066391, Time: 12202.77\n",
      "Iteration: 364000, Train loss: 0.0067912, Test loss: 0.0067912, Best test loss: 0.0066391, Time: 12233.05\n",
      "Iteration: 365000, Train loss: 0.0067524, Test loss: 0.0067524, Best test loss: 0.0066391, Time: 12263.26\n",
      "Iteration: 366000, Train loss: 0.0067563, Test loss: 0.0067563, Best test loss: 0.0066391, Time: 12293.44\n",
      "New best model saved at iteration 367000 with test MSE: 0.0065999\n",
      "Iteration: 367000, Train loss: 0.0065999, Test loss: 0.0065999, Best test loss: 0.0065999, Time: 12323.68\n",
      "New best model saved at iteration 368000 with test MSE: 0.0065231\n",
      "Iteration: 368000, Train loss: 0.0065231, Test loss: 0.0065231, Best test loss: 0.0065231, Time: 12353.87\n",
      "Iteration: 369000, Train loss: 0.0067000, Test loss: 0.0067000, Best test loss: 0.0065231, Time: 12384.11\n",
      "New best model saved at iteration 370000 with test MSE: 0.0064931\n",
      "Iteration: 370000, Train loss: 0.0064931, Test loss: 0.0064931, Best test loss: 0.0064931, Time: 12414.24\n",
      "Iteration: 371000, Train loss: 0.0066774, Test loss: 0.0066774, Best test loss: 0.0064931, Time: 12444.36\n",
      "Iteration: 372000, Train loss: 0.0067647, Test loss: 0.0067647, Best test loss: 0.0064931, Time: 12474.38\n",
      "Iteration: 373000, Train loss: 0.0066866, Test loss: 0.0066866, Best test loss: 0.0064931, Time: 12504.54\n",
      "Iteration: 374000, Train loss: 0.0069407, Test loss: 0.0069407, Best test loss: 0.0064931, Time: 12534.71\n",
      "Iteration: 375000, Train loss: 0.0065157, Test loss: 0.0065157, Best test loss: 0.0064931, Time: 12565.86\n",
      "Iteration: 376000, Train loss: 0.0066473, Test loss: 0.0066473, Best test loss: 0.0064931, Time: 12598.62\n",
      "Iteration: 377000, Train loss: 0.0071376, Test loss: 0.0071376, Best test loss: 0.0064931, Time: 12631.38\n",
      "New best model saved at iteration 378000 with test MSE: 0.0063965\n",
      "Iteration: 378000, Train loss: 0.0063965, Test loss: 0.0063965, Best test loss: 0.0063965, Time: 12664.12\n",
      "New best model saved at iteration 379000 with test MSE: 0.0063019\n",
      "Iteration: 379000, Train loss: 0.0063019, Test loss: 0.0063019, Best test loss: 0.0063019, Time: 12696.92\n",
      "Iteration: 380000, Train loss: 0.0068320, Test loss: 0.0068320, Best test loss: 0.0063019, Time: 12729.66\n",
      "Iteration: 381000, Train loss: 0.0064528, Test loss: 0.0064528, Best test loss: 0.0063019, Time: 12762.46\n",
      "Iteration: 382000, Train loss: 0.0063681, Test loss: 0.0063681, Best test loss: 0.0063019, Time: 12795.38\n",
      "Iteration: 383000, Train loss: 0.0065485, Test loss: 0.0065485, Best test loss: 0.0063019, Time: 12828.18\n",
      "Iteration: 384000, Train loss: 0.0068221, Test loss: 0.0068221, Best test loss: 0.0063019, Time: 12860.98\n",
      "Iteration: 385000, Train loss: 0.0064418, Test loss: 0.0064418, Best test loss: 0.0063019, Time: 12893.69\n",
      "Iteration: 386000, Train loss: 0.0063164, Test loss: 0.0063164, Best test loss: 0.0063019, Time: 12926.41\n",
      "Iteration: 387000, Train loss: 0.0067152, Test loss: 0.0067152, Best test loss: 0.0063019, Time: 12959.12\n",
      "Iteration: 388000, Train loss: 0.0064921, Test loss: 0.0064921, Best test loss: 0.0063019, Time: 12991.83\n",
      "Iteration: 389000, Train loss: 0.0067265, Test loss: 0.0067265, Best test loss: 0.0063019, Time: 13024.57\n",
      "Iteration: 390000, Train loss: 0.0066330, Test loss: 0.0066330, Best test loss: 0.0063019, Time: 13057.23\n",
      "Iteration: 391000, Train loss: 0.0063548, Test loss: 0.0063548, Best test loss: 0.0063019, Time: 13090.00\n",
      "Iteration: 392000, Train loss: 0.0065299, Test loss: 0.0065299, Best test loss: 0.0063019, Time: 13121.30\n",
      "Iteration: 393000, Train loss: 0.0063869, Test loss: 0.0063869, Best test loss: 0.0063019, Time: 13151.65\n",
      "Iteration: 394000, Train loss: 0.0064842, Test loss: 0.0064842, Best test loss: 0.0063019, Time: 13181.90\n",
      "Iteration: 395000, Train loss: 0.0066264, Test loss: 0.0066264, Best test loss: 0.0063019, Time: 13212.27\n",
      "Iteration: 396000, Train loss: 0.0066004, Test loss: 0.0066004, Best test loss: 0.0063019, Time: 13242.47\n",
      "Iteration: 397000, Train loss: 0.0066706, Test loss: 0.0066706, Best test loss: 0.0063019, Time: 13272.84\n",
      "Iteration: 398000, Train loss: 0.0069863, Test loss: 0.0069863, Best test loss: 0.0063019, Time: 13305.95\n",
      "Iteration: 399000, Train loss: 0.0066956, Test loss: 0.0066956, Best test loss: 0.0063019, Time: 13339.03\n",
      "Iteration: 400000, Train loss: 0.0065167, Test loss: 0.0065167, Best test loss: 0.0063019, Time: 13372.07\n",
      "Iteration: 401000, Train loss: 0.0063148, Test loss: 0.0063148, Best test loss: 0.0063019, Time: 13405.10\n",
      "Iteration: 402000, Train loss: 0.0066986, Test loss: 0.0066986, Best test loss: 0.0063019, Time: 13438.07\n",
      "Iteration: 403000, Train loss: 0.0065884, Test loss: 0.0065884, Best test loss: 0.0063019, Time: 13469.68\n",
      "Iteration: 404000, Train loss: 0.0063267, Test loss: 0.0063267, Best test loss: 0.0063019, Time: 13499.93\n",
      "Iteration: 405000, Train loss: 0.0063846, Test loss: 0.0063846, Best test loss: 0.0063019, Time: 13530.10\n",
      "Iteration: 406000, Train loss: 0.0064331, Test loss: 0.0064331, Best test loss: 0.0063019, Time: 13560.32\n",
      "Iteration: 407000, Train loss: 0.0063062, Test loss: 0.0063062, Best test loss: 0.0063019, Time: 13590.39\n",
      "Iteration: 408000, Train loss: 0.0065291, Test loss: 0.0065291, Best test loss: 0.0063019, Time: 13620.45\n",
      "Iteration: 409000, Train loss: 0.0064526, Test loss: 0.0064526, Best test loss: 0.0063019, Time: 13653.25\n",
      "Iteration: 410000, Train loss: 0.0065496, Test loss: 0.0065496, Best test loss: 0.0063019, Time: 13686.42\n",
      "Iteration: 411000, Train loss: 0.0064449, Test loss: 0.0064449, Best test loss: 0.0063019, Time: 13719.62\n",
      "Iteration: 412000, Train loss: 0.0065013, Test loss: 0.0065013, Best test loss: 0.0063019, Time: 13752.76\n"
     ]
    }
   ],
   "source": [
    "## Training of DeepONet\n",
    "start = time.time() # start time of training\n",
    "best_test_mse = float('inf')  # Initialize with infinity\n",
    "\n",
    "# Save initial model at 0th iteration\n",
    "save_model_params(params, resultdir, filename='model_params_best_deeponet.pkl')\n",
    "print(\"Saved initial model at iteration 0\")\n",
    "\n",
    "for iteration in range(n_epochs):\n",
    "    indices = jax.random.permutation(jax.random.PRNGKey(0), num_samples)\n",
    "    batch_index = indices[0:bs]\n",
    "    inputs_train_shuffled = inputs_train[batch_index]\n",
    "    outputs_train_shuffled = outputs_train[batch_index]\n",
    "    target_values = outputs_train_shuffled\n",
    "    branch_inputs = inputs_train_shuffled\n",
    "    trunk_inputs = grid\n",
    "    params, opt_state, value = resnet_update(params, branch_inputs, trunk_inputs, target_values, opt_state)\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        \n",
    "        params_branch, params_trunk = params\n",
    "        predictions = DeepONet(params, branch_inputs, trunk_inputs)\n",
    "        test_mse = jnp.mean((predictions - target_values)**2)\n",
    "\n",
    "        # Compare current test error with the best so far\n",
    "        if test_mse < best_test_mse:\n",
    "            best_test_mse = test_mse\n",
    "            # Save the model as it's the best so far\n",
    "            save_model_params(params, resultdir, filename='model_params_best_deeponet.pkl')\n",
    "            print(f\"New best model saved at iteration {iteration} with test MSE: {test_mse:.7f}\")\n",
    "\n",
    "        finish = time.time() - start\n",
    "        print(f\"Iteration: {iteration:3d}, Train loss: {objective(params, branch_inputs, trunk_inputs, target_values):.7f}, Test loss: {test_mse:.7f}, Best test loss: {best_test_mse:.7f}, Time: {finish:.2f}\")\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(objective(params, branch_inputs, trunk_inputs, target_values))\n",
    "    test_loss_list.append(test_mse)\n",
    "\n",
    "if save:\n",
    "    np.save(os.path.join(resultdir, 'iteration_list_deeponet.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'loss_list_deeponet.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list_deeponet.npy'), np.asarray(test_loss_list))\n",
    "\n",
    "# Plotting code remains the same\n",
    "plt.figure()\n",
    "plt.plot(iteration_list, loss_list, 'g', label='Training loss')\n",
    "plt.plot(iteration_list, test_loss_list, '-b', label='Test loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(resultdir, 'loss_plot_deeponet.pdf'))\n",
    "plt.show()\n",
    "plt.close()  \n",
    "  \n",
    "# end timer\n",
    "finish = time.time() - start\n",
    "print(\"Time (sec) to complete DeepONet training:\\n\" + str(finish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y6EIiKjo0JL8",
    "outputId": "b38f418a-6f26-48b2-da10-f06d9cde131e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model parameters\n",
      "Mean Squared Error Test :\n",
      " 13.819610233306884\n"
     ]
    }
   ],
   "source": [
    "# params_branch, params_trunk = params\n",
    "# Load the best model parameters\n",
    "best_params = load_model_params(resultdir, filename='model_params_best_deeponet.pkl')\n",
    "print(\"Loaded best model parameters\")\n",
    "\n",
    "# Predictions\n",
    "mse_list = []\n",
    "\n",
    "branch_inputs = inputs_test\n",
    "trunk_inputs = grid\n",
    "prediction = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval) \n",
    "\n",
    "inputs_save = inputs_test*inputs_std + inputs_mean\n",
    "outputs_save = outputs_test*outputs_std + outputs_mean\n",
    "prediction_save = prediction*outputs_std + outputs_mean\n",
    "\n",
    "save_dict = {'ground_motion': inputs_save, 'disp_pred': prediction_save,\\\n",
    "             'disp_target': outputs_save, 'grid': grid}\n",
    "\n",
    "io.savemat(resultdir+'/pred_deeponet_test.mat', save_dict)\n",
    "del prediction, inputs_save, outputs_save, prediction_save\n",
    "\n",
    "branch_inputs = inputs_train\n",
    "trunk_inputs = grid\n",
    "prediction = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval) \n",
    "\n",
    "inputs_save = inputs_train*inputs_std + inputs_mean\n",
    "outputs_save = outputs_train*outputs_std + outputs_mean\n",
    "prediction_save = prediction*outputs_std + outputs_mean\n",
    "\n",
    "save_dict = {'ground_motion': inputs_save, 'disp_pred': prediction_save,\\\n",
    "             'disp_target': outputs_save, 'grid': grid}\n",
    "\n",
    "io.savemat(resultdir+'/pred_deeponet_train.mat', save_dict)\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    branch_inputs = inputs_test[i].reshape(1, nx) \n",
    "    trunk_inputs = grid # (neval, 1) \n",
    "\n",
    "    prediction_i = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval)\n",
    "    target_i = outputs_test[i]\n",
    "\n",
    "    mse_i = np.mean((prediction_i - target_i)**2)\n",
    "    mse_list.append(mse_i.item())\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test :\\n\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.close(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(mse_list)\n",
    "hist, bins = np.histogram(data, bins=30)\n",
    "plt.figure()\n",
    "plt.hist(data, bins=bins, edgecolor='black')\n",
    "plt.xlabel('mean squared error')\n",
    "plt.ylabel('number of samples')\n",
    "plt.savefig(os.path.join(resultdir,'ErrorHistogram.pdf'))\n",
    "plt.show()\n",
    "plt.close"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5604392,
     "sourceId": 9262123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5620041,
     "sourceId": 9284520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
