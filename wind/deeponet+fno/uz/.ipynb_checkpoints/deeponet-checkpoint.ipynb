{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-TdAyUB0JL5"
   },
   "source": [
    "#### Designing a neural operator for mapping wind load to its corresponding response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiytiWyl0JL6",
    "outputId": "a6fe1eea-c14d-4d04-b88c-66444b33bbe4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 07:57:07.455539: W external/xla/xla/service/platform_util.cc:199] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 42481811456\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:879\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m   backend \u001b[38;5;241m=\u001b[39m \u001b[43m_init_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m   _backends[platform] \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:970\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    969\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform)\n\u001b[0;32m--> 970\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:668\u001b[0m, in \u001b[0;36mregister_plugin.<locals>.factory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxla_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m distribute_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m: distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mprocess_id,\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mnum_processes,\n\u001b[1;32m    673\u001b[0m }\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jaxlib/xla_client.py:200\u001b[0m, in \u001b[0;36mmake_c_api_client\u001b[0;34m(plugin_name, options, distributed_client)\u001b[0m\n\u001b[1;32m    199\u001b[0m   options \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed_client\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: no supported devices found for platform CUDA",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Check where gpu is enable or not\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xla_bridge\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mxla_bridge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mplatform)\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:1016\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# don't use util.memoize because there is no X64 dependence.\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\n\u001b[1;32m   1014\u001b[0m     platform: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient:\n\u001b[0;32m-> 1016\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:995\u001b[0m, in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    991\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m platform\n\u001b[1;32m    993\u001b[0m platform \u001b[38;5;241m=\u001b[39m (platform \u001b[38;5;129;01mor\u001b[39;00m _XLA_BACKEND\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m _PLATFORM_NAME\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 995\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[43mbackends\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m   platform \u001b[38;5;241m=\u001b[39m canonicalize_platform(platform)\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/xla_bridge.py:895\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 895\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_platforms\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy.random as npr\n",
    "from jax import jit, grad, vmap\n",
    "from jax.example_libraries.optimizers import adam\n",
    "from jax import value_and_grad\n",
    "from functools import partial\n",
    "from jax import jacfwd, jacrev\n",
    "import jax.nn as jnn\n",
    "import math\n",
    "from jax import random\n",
    "import jax\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from flax import linen as nn\n",
    "import sklearn.metrics\n",
    "from jax.lax import conv_general_dilated as conv_lax\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from termcolor import colored\n",
    "from scipy.io import loadmat\n",
    "import scipy.io as io\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check where gpu is enable or not\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaGMlhJl0JL7",
    "outputId": "5db466fa-897f-4405-969f-ad31658b8fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "cluster = False\n",
    "save = True\n",
    "\n",
    "if cluster == True:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-seed', dest='seed', type=int, default=0, help='Seed number.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Print all the arguments\n",
    "    for arg in vars(args):\n",
    "        print(f'{arg}: {getattr(args, arg)}')\n",
    "\n",
    "    seed = args.seed\n",
    "\n",
    "if cluster == False:\n",
    "    seed = 0 # Seed number.\n",
    "\n",
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(), 'Results')\n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "\n",
    "if save == True and cluster == True:\n",
    "    orig_stdout = sys.stdout\n",
    "    q = open(os.path.join(resultdir, 'outputs.txt'), 'w')\n",
    "    sys.stdout = q\n",
    "    print (\"------START------\")\n",
    "\n",
    "print('seed = '+str(seed))\n",
    "np.random.seed(seed)\n",
    "key = 1234 #random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVE4R7ox0JL7",
    "outputId": "74110ef4-29c0-4e48-911f-675b537590ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs_train: (800, 997, 4)\n",
      "Shape of inputs_test: (200, 997, 4)\n",
      "Shape of outputs_train: (800, 997)\n",
      "Shape of outputs_test: (200, 997)\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "index = loadmat('../../data/train_test_index.mat')\n",
    "train_index = index['train'][0,:].T\n",
    "test_index = index['test'][0,:].T\n",
    "\n",
    "cutoff = 100\n",
    "rtime = 30\n",
    "num_storeys = 37\n",
    "rstoreys = 1\n",
    "\n",
    "reader = loadmat('../../data/windData1.mat')\n",
    "x1 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y1 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "reader = loadmat('../../data/windData2.mat')\n",
    "x2 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y2 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "reader = loadmat('../../data/windData3.mat')\n",
    "x3 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y3 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "reader = loadmat('../../data/windData4.mat')\n",
    "x4 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y4 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "reader = loadmat('../../data/windData5.mat')\n",
    "x5 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y5 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "reader = loadmat('../../data/windData6.mat')\n",
    "x6 = reader['f'][:,0:-cutoff][:,::rtime, ::rstoreys]\n",
    "y6 = reader['uz'][:,cutoff::][:,::rtime, -1]\n",
    "\n",
    "x = np.concat((x1, x2, x3, x4, x5, x6), axis = 0)\n",
    "y = np.concat((y1, y2, y3, y4, y5, y6), axis = 0)\n",
    "t = np.array(np.linspace(0,1,997)).reshape(997,1)\n",
    "\n",
    "inputs_train = jnp.array(x[train_index])\n",
    "outputs_train = jnp.array(y[train_index])\n",
    "inputs_test = jnp.array(x[test_index])\n",
    "outputs_test = jnp.array(y[test_index])\n",
    "grid = jnp.array(t)\n",
    "#print(\"grid:\", grid)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the inputs\n",
    "inputs_mean = jnp.mean(inputs_train, axis = 0)\n",
    "input_std = jnp.std(inputs_train, axis = 0)\n",
    "outputs_mean = jnp.mean(outputs_train, axis = 0)\n",
    "outputs_std = jnp.std(outputs_train, axis = 0)\n",
    "\n",
    "inputs_train = (inputs_train - inputs_mean)/input_std\n",
    "outputs_train = (outputs_train - outputs_mean)/outputs_std\n",
    "inputs_test = (inputs_test - inputs_mean)/input_std\n",
    "outputs_test = (outputs_test - outputs_mean)/outputs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-UHALcEH0JL7"
   },
   "outputs": [],
   "source": [
    "# Initialize the Glorot (Xavier) normal distribution for weight initialization\n",
    "initializer = jax.nn.initializers.glorot_normal()\n",
    "rng_key = random.PRNGKey(0)\n",
    "key1, key2, key3, key4 = random.split(rng_key, 4)\n",
    "\n",
    "def conv(x, w, b):\n",
    "    \"\"\"Convolution operation with VALID padding\"\"\"\n",
    "    # Reshape inputs to match JAX's conv_general_dilated expectations\n",
    "    # x shape: (H, W, C_in)\n",
    "    # w shape: (H, W, C_in, C_out)\n",
    "    conv_out = conv_lax(\n",
    "        lhs=x[None, ...],  # Add batch dimension: (1, H, W, C_in)\n",
    "        rhs=w,             # Kernel: (H, W, C_in, C_out)\n",
    "        window_strides=(1, 1),\n",
    "        padding='VALID',\n",
    "        dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "    )\n",
    "    return conv_out[0] + b  # Remove batch dimension and add bias\n",
    "\n",
    "def init_cnn_params(p, key=random.PRNGKey(0)):\n",
    "    \"\"\"\n",
    "    Initialize CNN parameters for the branch network as a list of tuples.\n",
    "    Each tuple contains (weights, biases) for a layer.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples, each containing weights and biases for a layer\n",
    "    \"\"\"\n",
    "    key1, key2, key3, key4, key5 = random.split(key, 5)\n",
    "\n",
    "    conv1_w = random.normal(key1, (8, 8, 1, 6)) * 0.1\n",
    "    conv1_b = random.normal(key2, (6,)) * 0.1\n",
    "\n",
    "    conv2_w = random.normal(key2, (8, 8, 6, 6)) * 0.1\n",
    "    conv2_b = random.normal(key3, (1,)) * 0.1\n",
    "\n",
    "    conv3_w = random.normal(key3, (8, 8, 6, 6)) * 0.1\n",
    "    conv3_b = random.normal(key4, (6,)) * 0.1\n",
    "\n",
    "    flat_size = 93696 #TODO: change this to automatic\n",
    "\n",
    "    dense1_w = random.normal(key4, (flat_size, 256)) * jnp.sqrt(2.0 / flat_size)\n",
    "    dense1_b = jnp.zeros(256)\n",
    "\n",
    "    dense2_w = random.normal(key5, (256, p)) * jnp.sqrt(2.0 / 256)\n",
    "    dense2_b = jnp.zeros(p)\n",
    "\n",
    "    return [\n",
    "        (conv1_w, conv1_b),\n",
    "        (conv2_w, conv2_b),\n",
    "        (conv3_w, conv3_b),\n",
    "        (dense1_w, dense1_b),\n",
    "        (dense2_w, dense2_b)\n",
    "    ]\n",
    "\n",
    "def BranchNet(params, x):\n",
    "    \"\"\"\n",
    "    CNN-based branch network for the DeepONet.\n",
    "\n",
    "    Args:\n",
    "    params (list): List of tuples containing weights and biases\n",
    "    x (array): Input tensor of shape (batch_size, 41, 41)\n",
    "\n",
    "    Returns:\n",
    "    array: Output tensor of shape (batch_size, p)\n",
    "    \"\"\"\n",
    "    def single_forward(params, x):\n",
    "        # Unpack conv and dense layer parameters\n",
    "        (conv1_w, conv1_b), (conv2_w, conv2_b), (conv3_w, conv3_b), \\\n",
    "        (dense1_w, dense1_b), (dense2_w, dense2_b) = params\n",
    "        # Reshape input to (997, 37, 1) - adding channel dimension\n",
    "        x = x.reshape(997, 37, 1)\n",
    "\n",
    "        # Convolution layers with SiLU activation\n",
    "        x = jnn.silu(conv(x, conv1_w, conv1_b))\n",
    "        x = jnn.silu(conv(x, conv2_w, conv2_b))\n",
    "        x = jnn.silu(conv(x, conv3_w, conv3_b))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(-1)\n",
    "\n",
    "        # Dense layers\n",
    "        x = jnn.silu(jnp.dot(x, dense1_w) + dense1_b)\n",
    "        outputs = jnp.dot(x, dense2_w) + dense2_b\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    return vmap(partial(single_forward, params))(x)\n",
    "\n",
    "def init_glorot_params(layer_sizes, key = random.PRNGKey(seed)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the neural network using Glorot (Xavier) initialization.\n",
    "\n",
    "    Args:\n",
    "    layer_sizes (list): List of integers representing the size of each layer.\n",
    "    key (PRNGKey): Random number generator key for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples, each containing weights and biases for a layer.\n",
    "    \"\"\"\n",
    "    return [(initializer(key, (m, n), jnp.float32), jnp.zeros(n))\n",
    "            for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def TrunkNet(params, x):\n",
    "    \"\"\"\n",
    "    Implement the trunk network of the DeepONet.\n",
    "\n",
    "    Args:\n",
    "    params (list): List of weight and bias tuples for each layer.\n",
    "    x (float): First input to the trunk network.\n",
    "    t (float): Second input to the trunk network.\n",
    "\n",
    "    Returns:\n",
    "    array: Output of the trunk network.\n",
    "    \"\"\"\n",
    "    inputs = jnp.array(x)\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(x, w) + b\n",
    "        x = jnn.silu(outputs)\n",
    "    return outputs\n",
    "\n",
    "@jit\n",
    "def DeepONet(params, branch_inputs, trunk_inputs):\n",
    "    \"\"\"\n",
    "    Implement the complete DeepONet architecture.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Tuple containing branch and trunk network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "\n",
    "    Returns:\n",
    "    array: Output of the DeepONet.\n",
    "    \"\"\"\n",
    "    params_branch, params_trunk = params\n",
    "    branch_outputs = lambda x: BranchNet(params_branch, x)\n",
    "    b_out = branch_outputs(branch_inputs)\n",
    "    trunk_output = lambda y: TrunkNet(params_trunk, y)\n",
    "    t_out = trunk_output(trunk_inputs)\n",
    "    results = jnp.einsum('ik, lk -> il',b_out, t_out)\n",
    "    return results\n",
    "\n",
    "# network parameters.\n",
    "p = 300 # Number of output neurons in both the branch and trunk net outputs.\n",
    "nx = 101\n",
    "input_neurons_branch = nx # m\n",
    "input_neurons_trunk = 1\n",
    "\n",
    "layer_sizes_t = [input_neurons_trunk] + [100]*6 + [p]\n",
    "\n",
    "params_branch = init_cnn_params(p)\n",
    "params_trunk = init_glorot_params(layer_sizes=layer_sizes_t)\n",
    "\n",
    "params= (params_branch, params_trunk)\n",
    "\n",
    "def objective(params, branch_inputs, trunk_inputs, target_values):\n",
    "    \"\"\"\n",
    "    Define the objective function (loss function) for training.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Tuple containing branch and trunk network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "    target_values (array): True output values to compare against.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean squared error loss.\n",
    "    \"\"\"\n",
    "    predictions = DeepONet(params, branch_inputs, trunk_inputs)\n",
    "    loss_mse = jnp.mean((predictions - target_values)**2)\n",
    "    return loss_mse\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "@jit\n",
    "def update(params, branch_input, trunk_inputs, target_values, opt_state):\n",
    "    \"\"\"\n",
    "    Compute the gradient for a batch and update the parameters.\n",
    "\n",
    "    Args:\n",
    "    params (tuple): Current network parameters.\n",
    "    branch_inputs (array): Inputs for the branch network.\n",
    "    trunk_inputs (array): Inputs for the trunk network.\n",
    "    target_values (array): True output values.\n",
    "    opt_state: Current state of the optimizer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Updated parameters, updated optimizer state, and current loss value.\n",
    "    \"\"\"\n",
    "    value, grads = value_and_grad(objective)(params, branch_input, trunk_inputs, target_values)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "opt_init, opt_update, get_params = adam(step_size=1e-3, b1=0.9, b2=0.999, eps=1e-08)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "bs = 100 #batch size\n",
    "iteration_list, loss_list, test_loss_list = [], [], []\n",
    "iteration = 0\n",
    "\n",
    "n_epochs = 100000\n",
    "num_samples = len(inputs_train)\n",
    "\n",
    "# test input preparation\n",
    "branch_inputs_test = inputs_test\n",
    "targets = outputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NBe_opRm0JL8"
   },
   "outputs": [],
   "source": [
    "def save_model_params(params, resultdir, filename='model_params_deeponet.pkl'):\n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "\n",
    "    save_path = os.path.join(resultdir, filename)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "def load_model_params(resultdir, filename='model_params_deeponet.pkl'):\n",
    "    load_path = os.path.join(resultdir, filename)\n",
    "    with open(load_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    return params\n",
    "\n",
    "# Saving\n",
    "if save:\n",
    "    save_model_params(params, resultdir)\n",
    "\n",
    "# Loading (uncomment when needed)\n",
    "# model_params = load_model_params(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K_cWIcoG0JL8",
    "outputId": "14ed2ce1-0a8a-4013-b6ef-bde0813eba52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initial model at iteration 0\n",
      "(997, 37)\n",
      "(997, 37, 1)\n",
      "(990, 30, 6)\n",
      "(983, 23, 6)\n",
      "(976, 16, 6)\n",
      "(93696,)\n",
      "(300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 20:11:49.860962: W external/xla/xla/service/gpu/conv_algorithm_picker.cc:802] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.\n",
      "2024-12-02 20:11:49.860990: W external/xla/xla/service/gpu/conv_algorithm_picker.cc:805] Conv: (f32[100,6,990,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,1,997,37]{3,2,1,0}, f32[6,1,8,8]{3,2,1,0}, f32[6]{0}), window={size=8x8}, dim_labels=bf01_oi01->bf01, operand_precision={highest,highest}, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.9 = (f32[100,6,990,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,1,997,37]{3,2,1,0} %bitcast.1850, f32[6,1,8,8]{3,2,1,0} %transpose.92, f32[6]{0} %Arg_1.2), window={size=8x8}, dim_labels=bf01_oi01->bf01, operand_precision={highest,highest}, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_name=\"jit(update)/jit(main)/jvp(jit(DeepONet))/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=(Precision.HIGHEST, Precision.HIGHEST) preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1459770/2633924284.py\" source_line=11}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: INTERNAL: All algorithms tried for (f32[100,6,990,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,1,997,37]{3,2,1,0}, f32[6,1,8,8]{3,2,1,0}, f32[6]{0}), window={size=8x8}, dim_labels=bf01_oi01->bf01, operand_precision={highest,highest}, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} failed. Falling back to default algorithm.  Per-algorithm errors:\n  Profiling failure on cuDNN engine eng11{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng13{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng11{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m branch_inputs \u001b[38;5;241m=\u001b[39m inputs_train_shuffled\n\u001b[1;32m     16\u001b[0m trunk_inputs \u001b[38;5;241m=\u001b[39m grid\n\u001b[0;32m---> 17\u001b[0m params, opt_state, value \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrunk_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     params_branch, params_trunk \u001b[38;5;241m=\u001b[39m params\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/jax.venv/lib/python3.9/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.9 = (f32[100,6,990,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,1,997,37]{3,2,1,0} %bitcast.1850, f32[6,1,8,8]{3,2,1,0} %transpose.92, f32[6]{0} %Arg_1.2), window={size=8x8}, dim_labels=bf01_oi01->bf01, operand_precision={highest,highest}, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_name=\"jit(update)/jit(main)/jvp(jit(DeepONet))/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=(Precision.HIGHEST, Precision.HIGHEST) preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1459770/2633924284.py\" source_line=11}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: INTERNAL: All algorithms tried for (f32[100,6,990,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,1,997,37]{3,2,1,0}, f32[6,1,8,8]{3,2,1,0}, f32[6]{0}), window={size=8x8}, dim_labels=bf01_oi01->bf01, operand_precision={highest,highest}, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} failed. Falling back to default algorithm.  Per-algorithm errors:\n  Profiling failure on cuDNN engine eng11{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng13{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng11{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(6043): 'status'\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning."
     ]
    }
   ],
   "source": [
    "## Training of DeepONet\n",
    "start = time.time() # start time of training\n",
    "best_test_mse = float('inf')  # Initialize with infinity\n",
    "\n",
    "# Save initial model at 0th iteration\n",
    "save_model_params(params, resultdir, filename='model_params_best_deeponet.pkl')\n",
    "print(\"Saved initial model at iteration 0\")\n",
    "\n",
    "for iteration in range(n_epochs):\n",
    "    indices = jax.random.permutation(jax.random.PRNGKey(0), num_samples)\n",
    "    batch_index = indices[0:bs]\n",
    "    inputs_train_shuffled = inputs_train[batch_index]\n",
    "    outputs_train_shuffled = outputs_train[batch_index]\n",
    "    target_values = outputs_train_shuffled\n",
    "    branch_inputs = inputs_train_shuffled\n",
    "    trunk_inputs = grid\n",
    "    params, opt_state, value = update(params, branch_inputs, trunk_inputs, target_values, opt_state)\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        params_branch, params_trunk = params\n",
    "        predictions = DeepONet(params, branch_inputs, trunk_inputs)\n",
    "        test_mse = jnp.mean((predictions - target_values)**2)\n",
    "\n",
    "        # Compare current test error with the best so far\n",
    "        if test_mse < best_test_mse:\n",
    "            best_test_mse = test_mse\n",
    "            # Save the model as it's the best so far\n",
    "            save_model_params(params, resultdir, filename='model_params_best_deeponet.pkl')\n",
    "            print(f\"New best model saved at iteration {iteration} with test MSE: {test_mse:.7f}\")\n",
    "\n",
    "        finish = time.time() - start\n",
    "        print(f\"Iteration: {iteration:3d}, Train loss: {objective(params, branch_inputs, trunk_inputs, target_values):.7f}, Test loss: {test_mse:.7f}, Best test loss: {best_test_mse:.7f}, Time: {finish:.2f}\")\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(objective(params, branch_inputs, trunk_inputs, target_values))\n",
    "    test_loss_list.append(test_mse)\n",
    "\n",
    "if save:\n",
    "    np.save(os.path.join(resultdir, 'iteration_list_deeponet.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'loss_list_deeponet.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list_deeponet.npy'), np.asarray(test_loss_list))\n",
    "\n",
    "# Plotting code remains the same\n",
    "plt.figure()\n",
    "plt.plot(iteration_list, loss_list, 'g', label='Training loss')\n",
    "plt.plot(iteration_list, test_loss_list, '-b', label='Test loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if save:\n",
    "    plt.savefig(os.path.join(resultdir, 'loss_plot_deeponet.pdf'))\n",
    "\n",
    "# end timer\n",
    "finish = time.time() - start\n",
    "print(\"Time (sec) to complete:\\n\" + str(finish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y6EIiKjo0JL8",
    "outputId": "b38f418a-6f26-48b2-da10-f06d9cde131e"
   },
   "outputs": [],
   "source": [
    "# params_branch, params_trunk = params\n",
    "# Load the best model parameters\n",
    "best_params = load_model_params(resultdir, filename='model_params_best_deeponet.pkl')\n",
    "print(\"Loaded best model parameters\")\n",
    "\n",
    "# Predictions\n",
    "mse_list = []\n",
    "\n",
    "branch_inputs = inputs_test\n",
    "trunk_inputs = grid\n",
    "prediction = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval)\n",
    "\n",
    "inputs_save = inputs_test*input_std + inputs_mean\n",
    "outputs_save = outputs_test*outputs_std + outputs_mean\n",
    "prediction_save = prediction*outputs_std + outputs_mean\n",
    "\n",
    "save_dict = {'ground_motion': inputs_save, 'disp_pred': prediction_save,\\\n",
    "             'disp_target': outputs_save, 'grid': grid}\n",
    "\n",
    "io.savemat(resultdir+'/pred_deeponet_test.mat', save_dict)\n",
    "del prediction, inputs_save, outputs_save, prediction_save\n",
    "\n",
    "branch_inputs = inputs_train\n",
    "trunk_inputs = grid\n",
    "prediction = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval) \n",
    "\n",
    "inputs_save = inputs_train*input_std + inputs_mean\n",
    "outputs_save = outputs_train*outputs_std + outputs_mean\n",
    "prediction_save = prediction*outputs_std + outputs_mean\n",
    "\n",
    "save_dict = {'ground_motion': inputs_save, 'disp_pred': prediction_save,\\\n",
    "             'disp_target': outputs_save, 'grid': grid}\n",
    "\n",
    "io.savemat(resultdir+'/pred_deeponet_train.mat', save_dict)\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    branch_inputs = inputs_test[i].reshape(1, inputs_test[i].shape[0], inputs_test[i].shape[1]) \n",
    "    trunk_inputs = grid # (neval, 1) \n",
    "\n",
    "    prediction_i = DeepONet(best_params, branch_inputs, trunk_inputs) # (bs, neval)\n",
    "    target_i = outputs_test[i]\n",
    "\n",
    "    prediction_i = prediction_i*outputs_std + outputs_mean\n",
    "    target_i = target_i*outputs_std + outputs_mean\n",
    "\n",
    "    mse_i = np.mean((prediction_i - target_i)**2)\n",
    "    mse_list.append(mse_i.item())\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "\n",
    "        r2score = metrics.r2_score(target_i.flatten(), prediction_i.flatten())\n",
    "        relerror = np.linalg.norm(target_i- prediction_i) / np.linalg.norm(target_i)\n",
    "        r2score = float('%.4f'%r2score)\n",
    "        relerror = float('%.4f'%relerror)\n",
    "        print('Rel. L2 Error = '+str(relerror)+', R2 score = '+str(r2score))\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "        # Adjust subplot parameters for better spacing\n",
    "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.3)\n",
    "\n",
    "        # Input plot\n",
    "        ax = fig.add_subplot(1, 3, 1)\n",
    "        inputs_print = inputs_test[i]*input_std + inputs_mean\n",
    "        plt.plot(inputs_print[:,-1])\n",
    "        plt.title('Input', fontsize=14)\n",
    "\n",
    "        # Output plot\n",
    "        ax = fig.add_subplot(1, 3, 2)\n",
    "        target = target_i.reshape(grid.shape[0])\n",
    "        prediction = prediction_i.reshape(grid.shape[0])\n",
    "        plt.plot(target, color='blue', linewidth=2)\n",
    "        plt.plot(prediction, color='red', linewidth=2)\n",
    "        plt.title('Output Field', fontsize=14)\n",
    "        plt.legend(['Target', 'Prediction'])\n",
    "\n",
    "        # Error plot\n",
    "        ax = fig.add_subplot(1, 3, 3)\n",
    "        error = target - prediction\n",
    "        plt.plot(error, color='magenta')\n",
    "        plt.yscale(\"log\")  \n",
    "        plt.title('Absolute Error', fontsize=14)\n",
    "\n",
    "        print(colored('#'*230, 'green'))\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test :\\n\", mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5604392,
     "sourceId": 9262123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5620041,
     "sourceId": 9284520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (jax.venv)",
   "language": "python",
   "name": "jax.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
